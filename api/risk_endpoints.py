"""
API Endpoints pour le syst√®me de gestion des risques avanc√©
Fournit les m√©triques VaR/CVaR, corr√©lation, stress tests et monitoring temps r√©el
"""

from __future__ import annotations
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, date
from dataclasses import asdict, is_dataclass, replace
from decimal import Decimal
import math
import numpy as np

from fastapi import APIRouter, HTTPException, Query, Depends
from api.deps import get_required_user
from pydantic import BaseModel

from services.risk_management import risk_manager, RiskMetrics, CorrelationMatrix, StressTestResult, StressScenario, PerformanceAttribution, BacktestResult, RiskAlert, AlertSeverity, AlertCategory
from api.utils.cache import cache_get, cache_set
import os

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/risk", tags=["risk-management"])
COMPUTE_ON_STUB_SOURCES = (os.getenv("COMPUTE_ON_STUB_SOURCES", "false").strip().lower() == "true")

# Cache for risk endpoints (TTL: 30 min per CACHE_TTL_OPTIMIZATION.md)
_risk_cache = {}

# ===== Helper: Convert python/numpy/pandas types to JSON-safe natives =====
def _clean_for_json(obj: Any) -> Any:
    """Recursively convert complex types (numpy, dataclass, datetime) to JSON-safe values"""

    if obj is None or isinstance(obj, (str, bool)):
        return obj

    if isinstance(obj, (int, np.integer)):
        return int(obj)

    if isinstance(obj, (float, np.floating, Decimal)):
        value = float(obj)
        if math.isnan(value) or math.isinf(value):
            return None
        return value

    if isinstance(obj, (datetime, date)):
        return obj.isoformat()

    if isinstance(obj, np.ndarray):
        return [_clean_for_json(item) for item in obj.tolist()]

    if isinstance(obj, (list, tuple, set)):
        return [_clean_for_json(item) for item in obj]

    if is_dataclass(obj):
        return {key: _clean_for_json(value) for key, value in asdict(obj).items()}

    if hasattr(obj, "_asdict"):
        return {key: _clean_for_json(value) for key, value in obj._asdict().items()}

    if isinstance(obj, dict):
        return {str(key): _clean_for_json(value) for key, value in obj.items()}

    if hasattr(obj, "dict"):
        return {key: _clean_for_json(value) for key, value in obj.dict().items()}

    if hasattr(obj, "__dict__"):
        return {key: _clean_for_json(value) for key, value in obj.__dict__.items()}

    return obj

# ===== Helper: Score ‚Üí Level canonical mapping (docs/RISK_SEMANTICS.md) =====
def _score_to_risk_level(score: float) -> str:
    """
    Canonical mapping: Risk Score [0-100] ‚Üí Risk Level

    Source: docs/RISK_SEMANTICS.md + services/risk_scoring.py:score_to_level()
    Higher score = more robust = lower risk

    Returns: "very_low" | "low" | "medium" | "high" | "very_high" | "critical"
    """
    from services.risk_management import RiskLevel
    score = max(0, min(100, score))  # Clamp to [0, 100]

    if score >= 80:
        return RiskLevel.VERY_LOW
    elif score >= 65:
        return RiskLevel.LOW
    elif score >= 50:
        return RiskLevel.MEDIUM
    elif score >= 35:
        return RiskLevel.HIGH
    elif score >= 20:
        return RiskLevel.VERY_HIGH
    else:
        return RiskLevel.CRITICAL

# ===== Helper: Risk Score V2 (feature-flagged) =====
def _calculate_risk_score_v2(
    risk_metrics,
    exposure_by_group: Dict[str, float],
    group_risk_index: float,
    balances: List[Dict[str, Any]],
    correlation_metrics
) -> Tuple[float, Dict[str, Any]]:
    """Compute Risk Score V2 with VaR/CVaR + structure + GRI.
    Returns (score, breakdown dict).
    """
    def clamp(x, a, b):
        return max(a, min(b, x))

    score = 50.0
    breakdown: Dict[str, float] = {}

    # VaR/CVaR (positive values)
    var95 = float(getattr(risk_metrics, 'var_95_1d', 0.0) or 0.0)
    cvar95 = float(getattr(risk_metrics, 'cvar_95_1d', 0.0) or 0.0)
    d_var = -5.0 if var95 < 0.04 else (0.0 if var95 <= 0.08 else 5.0)
    d_cvar = -3.0 if cvar95 < 0.06 else (0.0 if cvar95 <= 0.12 else 3.0)
    score += d_var + d_cvar
    breakdown['var95'] = d_var
    breakdown['cvar95'] = d_cvar

    # Drawdown (use absolute)
    max_dd = abs(float(getattr(risk_metrics, 'max_drawdown', 0.0) or 0.0))
    if max_dd < 0.15:
        d_dd = -10.0
    elif max_dd <= 0.30:
        d_dd = 0.0
    elif max_dd <= 0.50:
        d_dd = 10.0
    else:
        d_dd = 20.0
    score += d_dd
    breakdown['drawdown'] = d_dd

    # Volatility annualized
    # ‚úÖ Option A semantics: Low volatility ‚Üí more robust ‚Üí score increases
    vol = float(getattr(risk_metrics, 'volatility_annualized', 0.0) or 0.0)
    if vol < 0.20:
        d_vol = +10.0   # Very low volatility ‚Üí score increases
    elif vol < 0.30:
        d_vol = +5.0    # Low volatility ‚Üí score increases
    elif vol <= 0.60:
        d_vol = 0.0     # Moderate volatility ‚Üí neutral
    elif vol <= 1.0:
        d_vol = -5.0    # High volatility ‚Üí score decreases
    else:
        d_vol = -10.0   # Very high volatility ‚Üí score decreases more
    score += d_vol
    breakdown['volatility'] = d_vol

    # Sharpe/Sortino (use the worse)
    # ‚úÖ Option A semantics: Good performance ‚Üí score increases (robustness)
    sharpe = float(getattr(risk_metrics, 'sharpe_ratio', 0.0) or 0.0)
    sortino = float(getattr(risk_metrics, 'sortino_ratio', sharpe) or sharpe)
    perf_ratio = min(sharpe, sortino)
    if perf_ratio < 0:
        d_perf = -15.0  # Negative ‚Üí score decreases (less robust)
    elif perf_ratio < 0.5:
        d_perf = -10.0  # Poor ‚Üí score decreases
    elif perf_ratio <= 1.0:
        d_perf = 0.0    # Neutral
    elif perf_ratio <= 1.5:
        d_perf = +5.0   # Good ‚Üí score increases
    elif perf_ratio <= 2.0:
        d_perf = +10.0  # Very good ‚Üí score increases more
    else:
        d_perf = +15.0  # Excellent ‚Üí score increases significantly
    score += d_perf
    breakdown['risk_adjusted_perf'] = d_perf

    # Structure: stables share from exposure_by_group
    stables_share = float(exposure_by_group.get('Stablecoins', 0.0) or 0.0)
    if stables_share >= 0.20:
        d_stables = -5.0
    elif stables_share >= 0.10:
        d_stables = -2.0
    elif stables_share < 0.05:
        d_stables = 3.0
    else:
        d_stables = 0.0
    score += d_stables
    breakdown['stables'] = d_stables

    # Concentration: Top5 and HHI
    try:
        total = sum(float(h.get('value_usd', 0.0) or 0.0) for h in balances)
        weights = []
        if total > 0:
            weights = sorted([(float(h.get('value_usd', 0.0))/total) for h in balances if float(h.get('value_usd', 0.0)) > 0], reverse=True)
        top5 = sum(weights[:5]) if weights else 0.0
        hhi = sum(w*w for w in weights)
    except Exception as e:
        logger.warning(f"Failed to calculate concentration metrics (top5/HHI): {e}")
        top5 = 0.0
        hhi = 0.0
    d_conc = 0.0
    if top5 > 0.80:
        d_conc += 5.0
    elif hhi > 0.20:
        d_conc += 3.0
    score += d_conc
    breakdown['concentration'] = d_conc

    # Diversification via diversification_ratio
    div_ratio = float(getattr(correlation_metrics, 'diversification_ratio', 1.0) or 1.0)
    d_div = 0.0 if div_ratio > 0.7 else (3.0 if div_ratio >= 0.4 else 6.0)
    score += d_div
    breakdown['diversification'] = d_div

    # Group Risk Index
    d_gri = min(10.0, max(0.0, (float(group_risk_index) - 4.0) * 2.0))
    score += d_gri
    breakdown['gri'] = d_gri

    score = clamp(score, 0.0, 100.0)
    return score, breakdown

class RiskMetricsResponse(BaseModel):
    """R√©ponse pour les m√©triques de risque"""
    success: bool
    risk_metrics: Optional[Dict[str, Any]] = None
    message: Optional[str] = None
    calculation_time: Optional[str] = None

class CorrelationResponse(BaseModel):
    """R√©ponse pour la matrice de corr√©lation"""
    success: bool
    correlation_matrix: Optional[Dict[str, Any]] = None
    message: Optional[str] = None
    calculation_time: Optional[str] = None

class StressTestResponse(BaseModel):
    """R√©ponse pour les stress tests"""
    success: bool
    stress_test_result: Optional[Dict[str, Any]] = None
    message: Optional[str] = None
    calculation_time: Optional[str] = None

class CustomStressRequest(BaseModel):
    """Requ√™te pour stress test personnalis√©"""
    asset_shocks: Dict[str, float]  # symbol/group -> shock percentage
    scenario_name: Optional[str] = "Custom Scenario"
    scenario_description: Optional[str] = "User-defined stress scenario"

class BacktestRequest(BaseModel):
    """Requ√™te pour backtest de strat√©gie"""
    strategy_name: str
    target_allocations: Dict[str, float]  # groupe -> allocation (ex: {"BTC": 0.4, "ETH": 0.3, "DeFi": 0.3})
    backtest_days: Optional[int] = 180
    rebalance_frequency_days: Optional[int] = 30
    transaction_cost_pct: Optional[float] = 0.001  # 0.1%

@router.get("/status")
async def get_risk_system_status():
    """
    Statut du syst√®me de gestion des risques
    """
    try:
        status = risk_manager.get_system_status()
        return {
            "success": True,
            "system_status": status,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting risk system status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/metrics", response_model=RiskMetricsResponse)
async def get_portfolio_risk_metrics(
    price_history_days: int = Query(30, ge=10, le=365, description="Number of days of history"),
    user: str = Depends(get_required_user)
):
    """
    Calcule les m√©triques de risque compl√®tes du portfolio

    Inclut:
    - VaR/CVaR √† 95% et 99%
    - Ratios Sharpe, Sortino, Calmar
    - Maximum Drawdown et Ulcer Index
    - Skewness et Kurtosis
    - Niveau de risque global

    Cache: 30 minutes TTL (per CACHE_TTL_OPTIMIZATION.md)
    """
    try:
        # PERFORMANCE FIX: Check cache first (30 min TTL)
        cache_key = f"risk_metrics:{user}:{price_history_days}"
        CACHE_TTL = 1800  # 30 minutes per CACHE_TTL_OPTIMIZATION.md

        cached = cache_get(_risk_cache, cache_key, CACHE_TTL)
        if cached is not None:
            logger.debug(f"Cache HIT for {cache_key}")
            return cached

        start_time = datetime.now()

        # Import des balances via le syst√®me unifi√© (m√™me source que /balances/current)
        from api.unified_data import get_unified_filtered_balances
        
        # R√©cup√©ration des holdings actuels via le syst√®me unifi√©
        balances_response = await get_unified_filtered_balances(source="cointracking", min_usd=1.0, user_id=user)
        src_used = (balances_response or {}).get('source_used', '')
        if (src_used.startswith('stub') or src_used == 'none') and not COMPUTE_ON_STUB_SOURCES:
            return RiskMetricsResponse(success=False, message="No real data: stub source in use")
        balances = balances_response.get('items', [])
        if not balances or len(balances) == 0:
            return RiskMetricsResponse(
                success=False,
                message="Aucun holding trouv√© dans le portfolio via le syst√®me unifi√©"
            )
        
        # Calcul des m√©triques de risque
        risk_metrics = await risk_manager.calculate_portfolio_risk_metrics(
            holdings=balances,
            price_history_days=price_history_days
        )
        
        # Conversion en dictionnaire pour API
        metrics_dict = {
            "var_95_1d": risk_metrics.var_95_1d,
            "var_99_1d": risk_metrics.var_99_1d,
            "cvar_95_1d": risk_metrics.cvar_95_1d,
            "cvar_99_1d": risk_metrics.cvar_99_1d,
            "volatility_annualized": risk_metrics.volatility_annualized,
            "sharpe_ratio": risk_metrics.sharpe_ratio,
            "sortino_ratio": risk_metrics.sortino_ratio,
            "calmar_ratio": risk_metrics.calmar_ratio,
            "max_drawdown": risk_metrics.max_drawdown,
            "max_drawdown_duration_days": risk_metrics.max_drawdown_duration_days,
            "current_drawdown": risk_metrics.current_drawdown,
            "ulcer_index": risk_metrics.ulcer_index,
            "skewness": risk_metrics.skewness,
            "kurtosis": risk_metrics.kurtosis,
            "overall_risk_level": risk_metrics.overall_risk_level.value,
            "risk_score": risk_metrics.risk_score,
            "calculation_date": risk_metrics.calculation_date.isoformat(),
            "data_points": risk_metrics.data_points,
            "confidence_level": risk_metrics.confidence_level
        }
        
        end_time = datetime.now()
        calculation_time = f"{(end_time - start_time).total_seconds():.2f}s"

        response = RiskMetricsResponse(
            success=True,
            risk_metrics=metrics_dict,
            calculation_time=calculation_time
        )

        # PERFORMANCE FIX: Cache the result for 30 minutes
        cache_set(_risk_cache, cache_key, response)
        logger.debug(f"Cache SET for {cache_key}")

        return response
        
    except Exception as e:
        logger.error(f"Erreur calcul m√©triques risque: {e}")
        return RiskMetricsResponse(
            success=False,
            message=f"Erreur lors du calcul: {str(e)}"
        )

@router.get("/correlation", response_model=CorrelationResponse)
async def get_correlation_matrix(
    lookback_days: int = Query(30, ge=10, le=365, description="Number of days for correlation calculation"),
    source: str = Query("cointracking", description="Data source: stub_balanced, cointracking, or cointracking_api"),
    user: str = Depends(get_required_user)
):
    """
    Calcule la matrice de corr√©lation temps r√©el entre assets
    
    Inclut:
    - Corr√©lations pairwise entre tous les assets
    - Analyse en composantes principales (PCA)
    - Ratio de diversification
    - Nombre effectif d'assets ind√©pendants
    """
    try:
        start_time = datetime.now()
        
        # Import des balances via le syst√®me unifi√© (m√™me source que /balances/current)
        from api.unified_data import get_unified_filtered_balances
        
        # R√©cup√©ration des holdings actuels via le syst√®me unifi√©
        balances_response = await get_unified_filtered_balances(source=source, min_usd=1.0, user_id=user)
        src_used = (balances_response or {}).get('source_used', '')
        if (src_used.startswith('stub') or src_used == 'none') and not COMPUTE_ON_STUB_SOURCES:
            return CorrelationResponse(success=False, message="No real data: stub source in use")
        balances = balances_response.get('items', [])
        logger.info(f"üîç Correlation endpoint: received {len(balances)} holdings from unified data source='{source}'")
        
        if not balances or len(balances) == 0:
            logger.warning(f"‚ùå No holdings found for correlation calculation with source='{source}'")
            return CorrelationResponse(
                success=False,
                message=f"Aucun holding trouv√© dans le portfolio via le syst√®me unifi√© (source: {source})"
            )
        
        # Calcul de la matrice de corr√©lation
        corr_matrix = await risk_manager.calculate_correlation_matrix(
            holdings=balances,
            lookback_days=lookback_days
        )
        
        # Conversion en dictionnaire pour API
        correlation_dict = {
            "correlations": corr_matrix.correlations,
            "eigen_values": corr_matrix.eigen_values,
            "eigen_vectors": corr_matrix.eigen_vectors,
            "principal_components": corr_matrix.principal_components,
            "diversification_ratio": corr_matrix.diversification_ratio,
            "effective_assets": corr_matrix.effective_assets,
            "last_updated": corr_matrix.last_updated.isoformat()
        }
        
        end_time = datetime.now()
        calculation_time = f"{(end_time - start_time).total_seconds():.2f}s"
        
        return CorrelationResponse(
            success=True,
            correlation_matrix=correlation_dict,
            calculation_time=calculation_time
        )
        
    except Exception as e:
        logger.error(f"Erreur calcul matrice corr√©lation: {e}")
        return CorrelationResponse(
            success=False,
            message=f"Erreur lors du calcul: {str(e)}"
        )

@router.get("/stress-test/{scenario}", response_model=StressTestResponse)
async def run_predefined_stress_test(
    scenario: str,
):
    """
    Ex√©cute un stress test bas√© sur des sc√©narios crypto historiques
    
    Sc√©narios disponibles:
    - bear_2018: Crash crypto 2018 (BTC -84%, Altcoins -95%)
    - covid_2020: Crash COVID Mars 2020 (BTC -50% en 2 semaines)
    - luna_2022: Effondrement Terra Luna (contagion DeFi)
    - ftx_2022: Bankruptcy FTX (crise de liquidit√©)
    """
    try:
        start_time = datetime.now()
        
        # Validation du sc√©nario
        scenario_mapping = {
            "bear_2018": StressScenario.BEAR_MARKET_2018,
            "covid_2020": StressScenario.COVID_CRASH_2020,
            "luna_2022": StressScenario.LUNA_COLLAPSE_2022,
            "ftx_2022": StressScenario.FTX_COLLAPSE_2022
        }
        
        if scenario not in scenario_mapping:
            available_scenarios = list(scenario_mapping.keys())
            return StressTestResponse(
                success=False,
                message=f"Sc√©nario invalide. Sc√©narios disponibles: {available_scenarios}"
            )
        
        # Import des balances CoinTracking
        from connectors.cointracking_api import get_current_balances
        
        # R√©cup√©ration des holdings actuels
        balances_response = await get_current_balances()
        balances = balances_response.get('items', []) if isinstance(balances_response, dict) else balances_response
        if not balances or len(balances) == 0:
            return StressTestResponse(
                success=False,
                message="Aucun holding trouv√© dans le portfolio"
            )
        
        # Ex√©cution du stress test
        stress_scenario = scenario_mapping[scenario]
        stress_result = await risk_manager.run_stress_test(
            holdings=balances,
            scenario=stress_scenario
        )
        
        # Conversion en dictionnaire pour API
        result_dict = {
            "scenario_name": stress_result.scenario_name,
            "scenario_description": stress_result.scenario_description,
            "portfolio_loss_pct": stress_result.portfolio_loss_pct,
            "portfolio_loss_usd": stress_result.portfolio_loss_usd,
            "worst_performing_assets": stress_result.worst_performing_assets,
            "best_performing_assets": stress_result.best_performing_assets,
            "var_breach": stress_result.var_breach,
            "recovery_time_estimate_days": stress_result.recovery_time_estimate_days,
            "risk_contribution": stress_result.risk_contribution
        }
        
        end_time = datetime.now()
        calculation_time = f"{(end_time - start_time).total_seconds():.2f}s"
        
        return StressTestResponse(
            success=True,
            stress_test_result=result_dict,
            calculation_time=calculation_time
        )
        
    except Exception as e:
        logger.error(f"Erreur stress test: {e}")
        return StressTestResponse(
            success=False,
            message=f"Erreur lors du stress test: {str(e)}"
        )

@router.post("/stress-test/custom", response_model=StressTestResponse)
async def run_custom_stress_test(
    request: CustomStressRequest
):
    """
    Ex√©cute un stress test personnalis√© avec shocks d√©finis par l'utilisateur
    
    Permet de d√©finir des shocks sp√©cifiques par asset ou groupe d'assets
    """
    try:
        start_time = datetime.now()
        
        # Validation des shocks
        if not request.asset_shocks:
            return StressTestResponse(
                success=False,
                message="Au moins un shock d'asset doit √™tre d√©fini"
            )
        
        # Import des balances CoinTracking
        from connectors.cointracking_api import get_current_balances
        
        # R√©cup√©ration des holdings actuels
        balances = await get_current_balances()
        if not balances or len(balances) == 0:
            return StressTestResponse(
                success=False,
                message="Aucun holding trouv√© dans le portfolio"
            )
        
        # Ex√©cution du stress test personnalis√©
        stress_result = await risk_manager.run_stress_test(
            holdings=balances,
            scenario=StressScenario.CUSTOM_SCENARIO,
            custom_shocks=request.asset_shocks
        )
        
        # Mise √† jour du nom et description
        stress_result.scenario_name = request.scenario_name
        stress_result.scenario_description = request.scenario_description
        
        # Conversion en dictionnaire pour API
        result_dict = {
            "scenario_name": stress_result.scenario_name,
            "scenario_description": stress_result.scenario_description,
            "portfolio_loss_pct": stress_result.portfolio_loss_pct,
            "portfolio_loss_usd": stress_result.portfolio_loss_usd,
            "worst_performing_assets": stress_result.worst_performing_assets,
            "best_performing_assets": stress_result.best_performing_assets,
            "var_breach": stress_result.var_breach,
            "recovery_time_estimate_days": stress_result.recovery_time_estimate_days,
            "risk_contribution": stress_result.risk_contribution
        }
        
        end_time = datetime.now()
        calculation_time = f"{(end_time - start_time).total_seconds():.2f}s"
        
        return StressTestResponse(
            success=True,
            stress_test_result=result_dict,
            calculation_time=calculation_time
        )
        
    except Exception as e:
        logger.error(f"Erreur stress test personnalis√©: {e}")
        return StressTestResponse(
            success=False,
            message=f"Erreur lors du stress test: {str(e)}"
        )

@router.get("/dashboard")
async def get_risk_dashboard(
    source: str = Query("cointracking", description="Data source: cointracking or cointracking_api"),
    pricing: str = Query("local", description="Price source: local or coingecko"),
    min_usd: float = Query(1.0, description="Minimum USD threshold"),
    price_history_days: int = Query(30, ge=10, le=365, description="History window for metrics (days)"),
    lookback_days: int = Query(30, ge=10, le=365, description="Window for correlations (days)"),
    user: str = Depends(get_required_user),
    risk_version: str = Query("v2_active", description="Risk Score version: legacy | v2_shadow | v2_active"),
    use_dual_window: bool = Query(True, description="Enable dual-window system (long-term + full intersection)"),
    min_history_days: int = Query(180, ge=90, le=365, description="Minimum days for long-term cohort"),
    min_coverage_pct: float = Query(0.80, ge=0.5, le=1.0, description="% minimum de valeur couverte pour cohorte"),
    min_asset_count: int = Query(5, ge=3, le=20, description="Minimum number of assets in cohort"),
    # üîß FIX: CSV hint for cache invalidation (Oct 2025)
    _csv_hint: Optional[str] = Query(None, description="Hint for cache invalidation when CSV changes (filename or timestamp)")
):
    """
    Endpoint pour dashboard de risque temps r√©el
    Combine toutes les m√©triques de risque en une seule r√©ponse
    """
    try:
        start_time = datetime.now()

        # Check cache (TTL: 30 min = 1800 seconds, optimized per CACHE_TTL_OPTIMIZATION.md)
        # üîß FIX: Include _csv_hint in cache key to invalidate when CSV changes (Oct 2025)
        csv_hint_part = f":{_csv_hint}" if _csv_hint else ""
        cache_key = f"risk_dashboard:{user}:{source}:{min_usd}:{risk_version}{csv_hint_part}"

        logger.info(f"üîç Risk dashboard request: user={user}, source={source}, csv_hint={_csv_hint}, cache_key={cache_key}")

        cached_result = cache_get(_risk_cache, cache_key, 1800)
        if cached_result:
            logger.info(f"‚úÖ Returning cached risk dashboard (cache hit)")
            return cached_result

        logger.info(f"‚ùå Cache miss, computing fresh metrics...")

        # R√©cup√©ration unifi√©e des balances (supporte stub | cointracking | cointracking_api)
        from api.unified_data import get_unified_filtered_balances
        unified = await get_unified_filtered_balances(source=source, min_usd=min_usd, user_id=user)
        balances = unified.get("items", [])
        source_used = unified.get("source_used", source)
        
        items_count = len(balances)
        
        if not balances or len(balances) == 0:
            return {
                "success": False,
                "message": "Aucun holding trouv√© dans le portfolio apr√®s filtrage"
            }
        
        # NOUVEAU: Utiliser le service centralis√© de m√©triques pour garantir la coh√©rence
        from services.portfolio_metrics import portfolio_metrics_service
        from services.price_history import get_cached_history
        import pandas as pd
        
        logger.info(f"üéØ Using centralized metrics service for {len(balances)} assets over {price_history_days} days")
        
        # R√©cup√©rer les donn√©es de prix historiques
        price_data = {}
        for balance in balances:
            symbol = balance.get('symbol', '').upper()
            if symbol:
                try:
                    prices = get_cached_history(symbol, days=price_history_days)
                    if prices and len(prices) > 10:
                        timestamps = [pd.Timestamp.fromtimestamp(p[0]) for p in prices]
                        values = [p[1] for p in prices]
                        price_data[symbol] = pd.Series(values, index=timestamps)
                except Exception as e:
                    logger.warning(f"Failed to get price data for {symbol}: {e}")
        
        if len(price_data) < 2:
            return {
                "success": False,
                "message": "Insufficient price data for metrics calculation"
            }
        
        # Cr√©er DataFrame des prix
        # ‚ö†Ô∏è NE PAS faire dropna() ici ! Le service dual-window a besoin des donn√©es compl√®tes
        # pour calculer s√©par√©ment la cohorte long-term (365j) et l'intersection full (61j)
        price_df = pd.DataFrame(price_data).ffill()

        # üÜï Phase 5: Shadow Mode - Calcul des 2 versions si n√©cessaire
        logger.info(f"üß™ SHADOW MODE DEBUG: risk_version received = '{risk_version}', use_dual_window = {use_dual_window}")
        compute_legacy = risk_version in ["legacy", "v2_shadow"]
        compute_v2 = risk_version in ["v2_shadow", "v2_active"]
        logger.info(f"üß™ SHADOW MODE DEBUG: compute_legacy = {compute_legacy}, compute_v2 = {compute_v2}")

        risk_metrics_legacy = None
        risk_metrics_v2 = None
        dual_window_result = None
        blend_metadata = None

        # Calcul LEGACY (sans blend, Long-Term pur)
        if compute_legacy:
            try:
                # Single window classique (ancien comportement)
                risk_metrics_legacy = portfolio_metrics_service.calculate_portfolio_metrics(
                    price_data=price_df,
                    balances=balances,
                    confidence_level=0.95
                )
                logger.info(f"üìä LEGACY Risk Score calculated: {risk_metrics_legacy.risk_score:.1f}")
            except Exception as e:
                logger.error(f"‚ùå Legacy calculation failed: {e}")
                risk_metrics_legacy = None

        # Calcul V2 (avec Dual-Window Blend + P√©nalit√©s)
        if compute_v2 and use_dual_window:
            try:
                dual_window_result = portfolio_metrics_service.calculate_dual_window_metrics(
                    price_data=price_df,
                    balances=balances,
                    min_history_days=min_history_days,
                    min_coverage_pct=min_coverage_pct,
                    min_asset_count=min_asset_count,
                    confidence_level=0.95
                )

                # üÜï BLEND DYNAMIQUE: Full Intersection prioritaire avec p√©nalit√©s
                long_term = dual_window_result.get('long_term')
                full_inter = dual_window_result['full_intersection']
                exclusions = dual_window_result.get('exclusions_metadata', {})

                # Crit√®res pour blend weight
                days_full = full_inter['window_days']
                # Coverage = % du portfolio couvert par Long-Term cohort (NOT Full Intersection!)
                coverage_long_term = long_term.get('coverage_pct', 0.0) if long_term else 0.0

                # Blend weight dynamique:
                # Si Long-Term coverage faible (beaucoup d'exclusions) ‚Üí priorit√© Full Intersection
                # Si Long-Term coverage √©lev√© (peu d'exclusions) ‚Üí blend √©quilibr√© Long-Term + Full
                #
                # Version simplifi√©e: w_long directement proportionnel √† coverage_LT
                w_long = coverage_long_term * 0.4  # Max 40% si coverage=100%
                w_full = 1 - w_long  # Donc w_full entre 0.6 et 1.0

                # P√©nalit√©s communes √† tous les cas
                excluded_pct = exclusions.get('excluded_pct', 0.0)
                penalty_excluded = -75 * max(0.0, (excluded_pct - 0.20) / 0.80) if excluded_pct > 0.20 else 0.0

                # P√©nalit√© memecoins jeunes (assets exclus de long-term qui sont des memes)
                excluded_assets = exclusions.get('excluded_assets', [])
                meme_keywords = ['PEPE', 'BONK', 'DOGE', 'SHIB', 'WIF', 'FLOKI']
                young_memes = [a for a in excluded_assets if any(kw in str(a.get('symbol', '')).upper() for kw in meme_keywords)]

                if young_memes and len(young_memes) >= 2:
                    # Calculer % valeur des memes jeunes
                    total_value = sum(float(b.get('value_usd', 0)) for b in balances)
                    young_memes_value = sum(float(a.get('value_usd', 0)) for a in young_memes)
                    young_memes_pct = young_memes_value / total_value if total_value > 0 else 0
                    penalty_memes_age = -min(25, 80 * young_memes_pct) if young_memes_pct > 0.30 else 0.0
                else:
                    penalty_memes_age = 0.0
                    young_memes_pct = 0.0

                # üÜï NOV 2025: Calculer exposure_by_group pour ajustements structurels
                from services.taxonomy import Taxonomy
                taxonomy = Taxonomy.load()

                # Bar√®me de risque par groupe (0-10)
                GROUP_RISK_LEVELS = {
                    'Stablecoins': 0, 'BTC': 2, 'ETH': 3, 'L2/Scaling': 5,
                    'DeFi': 5, 'AI/Data': 5, 'SOL': 6, 'L1/L0 majors': 6,
                    'Gaming/NFT': 6, 'Others': 7, 'Memecoins': 9
                }

                total_value = sum(float(b.get('value_usd', 0)) for b in balances)
                exposure_by_group = {group: 0.0 for group in GROUP_RISK_LEVELS.keys()}

                if total_value > 0:
                    for h in balances:
                        symbol = str(h.get('symbol', '')).upper()
                        group = taxonomy.group_for_alias(symbol)
                        w = float(h.get('value_usd', 0.0)) / total_value
                        exposure_by_group[group] = exposure_by_group.get(group, 0.0) + w

                # üÜï NOV 2025: Ajustements structurels bas√©s sur exposition r√©elle
                # (Protection Stables, Exposition Majors, Sur-exposition Altcoins)
                logger.debug(f"üìä Calculating structural adjustments from exposure: {exposure_by_group}")

                # 1. Protection Stablecoins (¬±15 pts) - Impact crash r√©el: -8% pertes √©vit√©es
                stables_pct = exposure_by_group.get("Stablecoins", 0.0)
                if stables_pct >= 0.15:
                    adj_stables = +15
                    logger.info(f"üõ°Ô∏è  Stablecoins bonus: +15 pts (excellent cushion: {stables_pct*100:.1f}%)")
                elif stables_pct >= 0.10:
                    adj_stables = +10
                    logger.info(f"üõ°Ô∏è  Stablecoins bonus: +10 pts (good protection: {stables_pct*100:.1f}%)")
                elif stables_pct >= 0.05:
                    adj_stables = +5
                    logger.info(f"üõ°Ô∏è  Stablecoins bonus: +5 pts (minimal protection: {stables_pct*100:.1f}%)")
                elif stables_pct > 0:
                    adj_stables = 0
                    logger.debug(f"üõ°Ô∏è  Stablecoins: no adjustment (insufficient: {stables_pct*100:.1f}%)")
                else:
                    adj_stables = -10
                    logger.warning(f"‚ö†Ô∏è  Stablecoins penalty: -10 pts (no protection: {stables_pct*100:.1f}%)")

                # 2. Exposition Majors (¬±10 pts) - BTC+ETH = stabilit√©
                majors_pct = exposure_by_group.get("BTC", 0.0) + exposure_by_group.get("ETH", 0.0)
                if majors_pct >= 0.60:
                    adj_majors = +10
                    logger.info(f"üèõÔ∏è  Majors bonus: +10 pts (healthy portfolio: {majors_pct*100:.1f}%)")
                elif majors_pct >= 0.50:
                    adj_majors = +5
                    logger.info(f"üèõÔ∏è  Majors bonus: +5 pts (acceptable: {majors_pct*100:.1f}%)")
                elif majors_pct >= 0.40:
                    adj_majors = 0
                    logger.debug(f"üèõÔ∏è  Majors: no adjustment (under-exposed: {majors_pct*100:.1f}%)")
                else:
                    adj_majors = -10
                    logger.warning(f"‚ö†Ô∏è  Majors penalty: -10 pts (risky: {majors_pct*100:.1f}%)")

                # 3. Sur-exposition Altcoins (-15 pts) - Volatilit√© x2-3 vs majors
                # Altcoins = tout sauf BTC, ETH, Stables, SOL (top 5)
                sol_pct = exposure_by_group.get("SOL", 0.0)
                altcoins_pct = max(0.0, 1.0 - majors_pct - stables_pct - sol_pct)
                if altcoins_pct > 0.50:
                    adj_altcoins = -15
                    logger.warning(f"‚ö†Ô∏è  Altcoins penalty: -15 pts (very risky: {altcoins_pct*100:.1f}%)")
                elif altcoins_pct > 0.40:
                    adj_altcoins = -10
                    logger.warning(f"‚ö†Ô∏è  Altcoins penalty: -10 pts (risky: {altcoins_pct*100:.1f}%)")
                elif altcoins_pct > 0.30:
                    adj_altcoins = -5
                    logger.info(f"‚ö†Ô∏è  Altcoins penalty: -5 pts (acceptable: {altcoins_pct*100:.1f}%)")
                else:
                    adj_altcoins = 0
                    logger.debug(f"‚úÖ Altcoins: no penalty (reasonable: {altcoins_pct*100:.1f}%)")

                # Total ajustements structurels
                adj_structural_total = adj_stables + adj_majors + adj_altcoins
                logger.info(f"üìä Structural adjustments: stables={adj_stables:+d}, majors={adj_majors:+d}, altcoins={adj_altcoins:+d} ‚Üí TOTAL={adj_structural_total:+d}")

                # CAS 1: Blend Long-Term + Full Intersection
                if long_term and days_full >= 120 and coverage_long_term >= 0.80:
                    logger.info(f"‚úÖ BLEND MODE: Full={w_full:.2f}, Long={w_long:.2f} ({days_full}d, LT coverage={coverage_long_term*100:.0f}%)")

                    # Blend Risk Score
                    risk_score_full = full_inter['metrics'].risk_score
                    risk_score_long = long_term['metrics'].risk_score
                    blended_risk_score = w_full * risk_score_full + w_long * risk_score_long

                    # Appliquer p√©nalit√©s + ajustements structurels
                    final_risk_score = max(0, min(100, blended_risk_score + penalty_excluded + penalty_memes_age + adj_structural_total))

                    logger.info(f"üìä Risk Score V2 blend: full={risk_score_full:.1f}, long={risk_score_long:.1f}, blended={blended_risk_score:.1f}")
                    logger.info(f"‚ö†Ô∏è  Penalties: excluded={penalty_excluded:.1f}, young_memes={penalty_memes_age:.1f} ({len(young_memes)} memes)")
                    logger.info(f"üÜï Structural adjustments: {adj_structural_total:+.1f} (stables={adj_stables:+d}, majors={adj_majors:+d}, altcoins={adj_altcoins:+d})")
                    logger.info(f"‚úÖ Final Risk Score V2: {final_risk_score:.1f} (was {blended_risk_score:.1f} before all adjustments)")

                    # ‚úÖ Stocker m√©triques v2 AVEC Risk Score V2 = Blend + P√©nalit√©s
                    risk_metrics_v2 = replace(full_inter['metrics'], risk_score=final_risk_score)

                    # üÜï RECALCULER overall_risk_level √† partir du nouveau score
                    risk_metrics_v2 = replace(risk_metrics_v2, overall_risk_level=_score_to_risk_level(final_risk_score))

                    # Sharpe blend√© (pour coh√©rence)
                    sharpe_blended = w_full * full_inter['metrics'].sharpe_ratio + w_long * long_term['metrics'].sharpe_ratio
                    risk_metrics_v2 = replace(risk_metrics_v2, sharpe_ratio=sharpe_blended)

                    # üÜï Phase 5: M√©tadonn√©es blend pour API response
                    blend_metadata = {
                        "mode": "blend",
                        "w_full": w_full,
                        "w_long": w_long,
                        "risk_score_full": risk_score_full,
                        "risk_score_long": risk_score_long,
                        "blended_risk_score": blended_risk_score,
                        "penalty_excluded": penalty_excluded,
                        "penalty_memes": penalty_memes_age,
                        "adj_stables": adj_stables,
                        "adj_majors": adj_majors,
                        "adj_altcoins": adj_altcoins,
                        "adj_structural_total": adj_structural_total,
                        "final_risk_score_v2": final_risk_score,
                        "young_memes_count": len(young_memes),
                        "young_memes_pct": young_memes_pct,
                        "excluded_pct": excluded_pct,
                        "sharpe_used": {
                            "full": full_inter['metrics'].sharpe_ratio,
                            "long": long_term['metrics'].sharpe_ratio,
                            "blended": sharpe_blended
                        }
                    }

                # CAS 2: Long-Term uniquement (Full insuffisante) AVEC p√©nalit√©s
                elif long_term:
                    logger.info(f"‚úÖ Using LONG-TERM window: {long_term['window_days']}d, {long_term['asset_count']} assets (Full insufficient)")

                    base_risk_score = long_term['metrics'].risk_score
                    final_risk_score = max(0, min(100, base_risk_score + penalty_excluded + penalty_memes_age + adj_structural_total))

                    logger.info(f"üìä Risk Score V2 (Long-Term only): base={base_risk_score:.1f}, penalties={penalty_excluded + penalty_memes_age:.1f}, structural={adj_structural_total:+.1f}, final={final_risk_score:.1f}")
                    logger.info(f"‚ö†Ô∏è  Penalties: excluded={penalty_excluded:.1f}, young_memes={penalty_memes_age:.1f} ({len(young_memes)} memes)")
                    logger.info(f"üÜï Structural adjustments: {adj_structural_total:+.1f} (stables={adj_stables:+d}, majors={adj_majors:+d}, altcoins={adj_altcoins:+d})")

                    # ‚úÖ Stocker m√©triques v2 avec p√©nalit√©s
                    risk_metrics_v2 = replace(long_term['metrics'], risk_score=final_risk_score)

                    # üÜï RECALCULER overall_risk_level √† partir du nouveau score
                    risk_metrics_v2 = replace(risk_metrics_v2, overall_risk_level=_score_to_risk_level(final_risk_score))

                    blend_metadata = {
                        "mode": "long_term_only",
                        "w_full": 0.0,
                        "w_long": 1.0,
                        "risk_score_full": full_inter['metrics'].risk_score,
                        "risk_score_long": base_risk_score,
                        "blended_risk_score": base_risk_score,
                        "penalty_excluded": penalty_excluded,
                        "penalty_memes": penalty_memes_age,
                        "adj_stables": adj_stables,
                        "adj_majors": adj_majors,
                        "adj_altcoins": adj_altcoins,
                        "adj_structural_total": adj_structural_total,
                        "final_risk_score_v2": final_risk_score,
                        "young_memes_count": len(young_memes),
                        "young_memes_pct": young_memes_pct,
                        "excluded_pct": excluded_pct,
                        "sharpe_used": {
                            "full": full_inter['metrics'].sharpe_ratio,
                            "long": long_term['metrics'].sharpe_ratio,
                            "blended": long_term['metrics'].sharpe_ratio
                        }
                    }

                # CAS 3: Full Intersection uniquement (pas de cohorte long-term) AVEC p√©nalit√©s
                else:
                    logger.warning(f"‚ö†Ô∏è  Using FULL INTERSECTION only: {days_full}d (no long-term cohort)")

                    base_risk_score = full_inter['metrics'].risk_score
                    final_risk_score = max(0, min(100, base_risk_score + penalty_excluded + penalty_memes_age + adj_structural_total))

                    logger.info(f"üìä Risk Score V2 (Full only): base={base_risk_score:.1f}, penalties={penalty_excluded + penalty_memes_age:.1f}, structural={adj_structural_total:+.1f}, final={final_risk_score:.1f}")
                    logger.info(f"‚ö†Ô∏è  Penalties: excluded={penalty_excluded:.1f}, young_memes={penalty_memes_age:.1f} ({len(young_memes)} memes)")
                    logger.info(f"üÜï Structural adjustments: {adj_structural_total:+.1f} (stables={adj_stables:+d}, majors={adj_majors:+d}, altcoins={adj_altcoins:+d})")

                    # ‚úÖ Stocker m√©triques v2 avec p√©nalit√©s
                    risk_metrics_v2 = replace(full_inter['metrics'], risk_score=final_risk_score)

                    # üÜï RECALCULER overall_risk_level √† partir du nouveau score
                    risk_metrics_v2 = replace(risk_metrics_v2, overall_risk_level=_score_to_risk_level(final_risk_score))

                    # M√©tadonn√©es pour API
                    blend_metadata = {
                        "mode": "full_intersection_only",
                        "w_full": 1.0,
                        "w_long": 0.0,
                        "risk_score_full": base_risk_score,
                        "risk_score_long": None,
                        "blended_risk_score": base_risk_score,
                        "penalty_excluded": penalty_excluded,
                        "penalty_memes": penalty_memes_age,
                        "adj_stables": adj_stables,
                        "adj_majors": adj_majors,
                        "adj_altcoins": adj_altcoins,
                        "adj_structural_total": adj_structural_total,
                        "final_risk_score_v2": final_risk_score,
                        "young_memes_count": len(young_memes),
                        "young_memes_pct": young_memes_pct,
                        "excluded_pct": excluded_pct,
                        "sharpe_used": {
                            "full": full_inter['metrics'].sharpe_ratio,
                            "long": None,
                            "blended": full_inter['metrics'].sharpe_ratio
                        }
                    }

            except Exception as e:
                # ‚úÖ FIX: Differentiate data quality issues (WARNING) from real errors (ERROR)
                error_msg = str(e)
                if "Insufficient data points" in error_msg or "sparse price coverage" in error_msg:
                    logger.warning(f"‚ö†Ô∏è Dual window V2 data quality issue: {e}")
                else:
                    logger.error(f"‚ùå Dual window V2 calculation failed: {e}")
                risk_metrics_v2 = None

        # Fallback si V2 demand√© mais √©chec
        if compute_v2 and risk_metrics_v2 is None:
            logger.warning("‚ö†Ô∏è  V2 requested but failed, falling back to legacy calculation")
            risk_metrics_v2 = portfolio_metrics_service.calculate_portfolio_metrics(
                price_data=price_df,
                balances=balances,
                confidence_level=0.95
            )

        # D√©terminer quel score est "actif" (utilis√© pour d√©cisions)
        if risk_version == "v2_active":
            risk_metrics = risk_metrics_v2 if risk_metrics_v2 else risk_metrics_legacy
            active_version = "v2"
        else:
            # legacy ou v2_shadow ‚Üí legacy est actif
            risk_metrics = risk_metrics_legacy if risk_metrics_legacy else risk_metrics_v2
            active_version = "legacy"

        # Fallback ultime si aucun calcul n'a r√©ussi
        if risk_metrics is None:
            return {
                "success": False,
                "message": "Failed to calculate risk metrics (both legacy and v2 failed)"
            }
        
        # Calculer les m√©triques de corr√©lation
        correlation_metrics = portfolio_metrics_service.calculate_correlation_metrics(
            price_data=price_df,
            min_correlation_threshold=0.7
        )

        # Exposition par groupes (via Taxonomy) + Group Risk Index (GRI)
        total_value = sum(float(h.get("value_usd", 0.0)) for h in balances) or 0.0
        from services.taxonomy import Taxonomy
        taxonomy = Taxonomy.load()

        # Bar√®me de risque par groupe (0-10), simple et explicable
        GROUP_RISK_LEVELS = {
            'Stablecoins': 0,
            'BTC': 2,
            'ETH': 3,
            'L2/Scaling': 5,
            'DeFi': 5,
            'AI/Data': 5,
            'SOL': 6,
            'L1/L0 majors': 6,
            'Gaming/NFT': 6,
            'Others': 7,
            'Memecoins': 9,
        }

        # Initialize exposure_by_group with ALL 11 canonical groups at 0.0
        # This ensures all groups appear in API response, even if portfolio has 0% in some
        exposure_by_group = {group: 0.0 for group in GROUP_RISK_LEVELS.keys()}

        # Add actual exposures
        if total_value > 0:
            for h in balances:
                symbol = str(h.get('symbol', '')).upper()
                group = taxonomy.group_for_alias(symbol)
                w = float(h.get('value_usd', 0.0)) / total_value
                exposure_by_group[group] = exposure_by_group.get(group, 0.0) + w
        if exposure_by_group:
            gri_raw = 0.0
            for g, w in exposure_by_group.items():
                level = GROUP_RISK_LEVELS.get(g, 6)  # d√©faut mod√©r√© si inconnu
                gri_raw += w * level
            group_risk_index = max(0.0, min(10.0, gri_raw))
        else:
            group_risk_index = 0.0

        # Construction de la r√©ponse dashboard avec m√©triques centralis√©es
        
        # ‚úÖ Utiliser le Risk Score autoritaire du service (docs/RISK_SEMANTICS.md)
        # Le service calcule d√©j√† risk_score (robustesse 0-100) et overall_risk_level (enum)
        # avec la s√©mantique correcte : score √©lev√© = robuste = risque faible
        risk_score_authoritative = risk_metrics.risk_score
        overall_risk_level = getattr(risk_metrics.overall_risk_level, "value", str(risk_metrics.overall_risk_level))

        # üÜï Phase 4: Calculer les 2 versions du Structural Score si n√©cessaire
        from services.risk.structural_score_v2 import compute_structural_score_v2, get_structural_level

        # Variables pour shadow mode
        risk_score_structural_legacy = None
        structural_breakdown_legacy = None
        structural_score_v2 = None
        structural_breakdown_v2 = None

        # Inputs communs pour les 2 calculs
        total_value = sum(float(h.get("value_usd", 0.0)) for h in balances) or 1.0
        hhi = sum((float(h.get("value_usd", 0.0)) / total_value) ** 2 for h in balances)
        memes_pct = exposure_by_group.get('Memecoins', 0.0)
        effective_assets = getattr(correlation_metrics, 'effective_assets', len(balances))

        # Calcul LEGACY Structural (utilise toujours risk_metrics, qui est d√©j√† s√©lectionn√©)
        if risk_metrics:
            risk_score_result = _calculate_risk_score_v2(
                risk_metrics=risk_metrics,
                exposure_by_group=exposure_by_group,
                group_risk_index=group_risk_index,
                balances=balances,
                correlation_metrics=correlation_metrics
            )
            risk_score_structural_legacy = risk_score_result[0]
            structural_breakdown_legacy = risk_score_result[1]

        # Calcul V2 Structural (toujours, car c'est la nouvelle formule)
        structural_score_v2, structural_breakdown_v2 = compute_structural_score_v2(
            hhi=hhi,
            memes_pct=memes_pct,
            gri=group_risk_index,
            effective_assets=effective_assets,
            total_value=total_value
        )

        # Pour compatibilit√©, garder l'ancien nom pour la r√©ponse par d√©faut
        risk_score_structural = structural_score_v2 if risk_version == "v2_active" else (risk_score_structural_legacy or structural_score_v2)
        structural_breakdown = structural_breakdown_v2 if risk_version == "v2_active" else (structural_breakdown_legacy or structural_breakdown_v2)

        # üÜï DEBUG: Log avant cr√©ation du dict
        logger.info(f"üß™ SHADOW V2 DEBUG: About to create dashboard_data with risk_version={risk_version}, active_version={active_version}, structural_score_v2={structural_score_v2}")

        dashboard_data = {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "portfolio_summary": {
                "total_value": sum(h.get("value_usd", 0) for h in balances),
                "num_assets": len(balances),
                "confidence_level": risk_metrics.confidence_level
            },
            "risk_metrics": {
                # ‚ö° M√âTRIQUES CENTRALIS√âES - Coh√©rentes avec tous les modules
                "var_95_1d": risk_metrics.var_95_1d,
                "var_99_1d": risk_metrics.var_99_1d,
                "cvar_95_1d": risk_metrics.cvar_95_1d,
                "cvar_99_1d": risk_metrics.cvar_99_1d,
                "volatility_annualized": risk_metrics.volatility_annualized,
                "sharpe_ratio": risk_metrics.sharpe_ratio,
                "sortino_ratio": risk_metrics.sortino_ratio,
                "calmar_ratio": risk_metrics.calmar_ratio,
                "max_drawdown": risk_metrics.max_drawdown,
                "max_drawdown_duration_days": risk_metrics.max_drawdown_duration_days,
                "current_drawdown": risk_metrics.current_drawdown,
                "ulcer_index": risk_metrics.ulcer_index,
                "skewness": risk_metrics.skewness,
                "kurtosis": risk_metrics.kurtosis,
                # ‚úÖ Scores et niveau autoritaires (source de v√©rit√©)
                "overall_risk_level": overall_risk_level,
                "risk_score": risk_score_authoritative,        # Autoritaire (VaR + Sharpe + DD + Vol) - ACTIF
                "risk_score_structural": risk_score_structural,  # Structurel (+ GRI + Concentration)
                "structural_breakdown": structural_breakdown,    # D√©tail contributions (audit)
                # üÜï Phase 5 + 4: Shadow Mode - Version info (Risk + Structure)
                "risk_version_info": (lambda: _clean_for_json({
                    "active_version": active_version,                                      # legacy | v2
                    "requested_version": risk_version,                                     # legacy | v2_shadow | v2_active
                    # Risk Score (VaR + Sharpe + DD + Vol) - Performance de march√©
                    "risk_score_legacy": risk_metrics_legacy.risk_score if risk_metrics_legacy else None,
                    "risk_score_v2": risk_metrics_v2.risk_score if risk_metrics_v2 else None,
                    "sharpe_legacy": risk_metrics_legacy.sharpe_ratio if risk_metrics_legacy else None,
                    "sharpe_v2": risk_metrics_v2.sharpe_ratio if risk_metrics_v2 else None,
                    # üÜï Portfolio Structure Score - Structure pure (HHI, memes, GRI, diversit√©)
                    "portfolio_structure_score": structural_score_v2,
                    "structure_breakdown": structural_breakdown_v2,
                    "structure_label": "structure_pure",
                    # Integrated Structural (legacy) - Mix structure + performance
                    "integrated_structural_legacy": risk_score_structural_legacy,
                    "integrated_breakdown_legacy": structural_breakdown_legacy,
                    "integrated_label": "structure_plus_performance",
                    # M√©tadonn√©es blend v2 (si disponible)
                    "blend_metadata": blend_metadata
                }))() if risk_version in ["v2_shadow", "v2_active"] else None,
                # Exposition par groupes et indice GRI (0-10)
                "exposure_by_group": exposure_by_group,
                "group_risk_index": group_risk_index,
                "calculation_date": risk_metrics.calculation_date.isoformat(),
                "data_points": risk_metrics.data_points,
                "confidence_level": risk_metrics.confidence_level,
                # ‚úÖ Metadata fen√™tres temporelles (tra√ßabilit√© + dual-window)
                "window_used": {
                    "price_history_days": price_history_days,
                    "lookback_days": lookback_days,
                    "actual_data_points": risk_metrics.data_points,
                    # üÜï Dual Window Metadata
                    "dual_window_enabled": use_dual_window and dual_window_result is not None,
                    "risk_score_source": dual_window_result['risk_score_source'] if dual_window_result else 'single_window'
                },
                # üÜï Dual Window Details (si activ√©)
                "dual_window": {
                    "enabled": use_dual_window and dual_window_result is not None,
                    "long_term": {
                        "available": dual_window_result['long_term'] is not None if dual_window_result else False,
                        "window_days": dual_window_result['long_term']['window_days'] if dual_window_result and dual_window_result['long_term'] else None,
                        "asset_count": dual_window_result['long_term']['asset_count'] if dual_window_result and dual_window_result['long_term'] else None,
                        "coverage_pct": dual_window_result['long_term']['coverage_pct'] if dual_window_result and dual_window_result['long_term'] else None,
                        "metrics": {
                            "sharpe_ratio": dual_window_result['long_term']['metrics'].sharpe_ratio if dual_window_result and dual_window_result['long_term'] else None,
                            "volatility": dual_window_result['long_term']['metrics'].volatility_annualized if dual_window_result and dual_window_result['long_term'] else None,
                            "risk_score": dual_window_result['long_term']['metrics'].risk_score if dual_window_result and dual_window_result['long_term'] else None
                        } if dual_window_result and dual_window_result['long_term'] else None
                    } if dual_window_result else None,
                    "full_intersection": {
                        "window_days": dual_window_result['full_intersection']['window_days'] if dual_window_result else None,
                        "asset_count": dual_window_result['full_intersection']['asset_count'] if dual_window_result else None,
                        "metrics": {
                            "sharpe_ratio": dual_window_result['full_intersection']['metrics'].sharpe_ratio if dual_window_result else None,
                            "volatility": dual_window_result['full_intersection']['metrics'].volatility_annualized if dual_window_result else None,
                            "risk_score": dual_window_result['full_intersection']['metrics'].risk_score if dual_window_result else None
                        } if dual_window_result else None
                    } if dual_window_result else None,
                    "exclusions": dual_window_result['exclusions_metadata'] if dual_window_result else None
                } if use_dual_window else None
            },
            "correlation_metrics": {
                "diversification_ratio": correlation_metrics.diversification_ratio,
                "effective_assets": correlation_metrics.effective_assets,
                "top_correlations": correlation_metrics.top_correlations
            },
            "alerts": _generate_centralized_risk_alerts(risk_metrics, correlation_metrics)
        }
        
        # Log d√©taill√© avec info dual-window
        dual_info = ""
        if dual_window_result:
            if dual_window_result['long_term']:
                dual_info = f" [Dual-Window: LT={dual_window_result['long_term']['window_days']}d/{dual_window_result['long_term']['asset_count']}assets, FI={dual_window_result['full_intersection']['window_days']}d/{dual_window_result['full_intersection']['asset_count']}assets]"
            else:
                dual_info = f" [Dual-Window: FI fallback only]"

        logger.info(f"‚úÖ Centralized metrics calculated: Sharpe={risk_metrics.sharpe_ratio:.2f}, Vol={risk_metrics.volatility_annualized:.2f}, MaxDD={risk_metrics.max_drawdown:.2%}, RiskScore={risk_score_authoritative:.1f}, RiskStructural={risk_score_structural:.1f}{dual_info}")

        end_time = datetime.now()
        calculation_time = f"{(end_time - start_time).total_seconds():.2f}s"
        dashboard_data["calculation_time"] = calculation_time

        # Add normalized metadata for frontend traceability
        import hashlib
        taxonomy_version = "v2"  # Current taxonomy version
        # Simple hash based on groups used for consistency checking (non-cryptographic)
        groups_hash = hashlib.md5(",".join(sorted(exposure_by_group.keys())).encode(), usedforsecurity=False).hexdigest()[:8]

        dashboard_data["meta"] = {
            "user_id": user,
            "source_id": source_used,
            "taxonomy_version": taxonomy_version,
            "taxonomy_hash": groups_hash,
            "generated_at": datetime.now().isoformat(),
            "correlation_id": f"risk-{user}-{int(datetime.now().timestamp())}"
        }

        # Log metadata for traceability
        logger.info(f"üè∑Ô∏è Risk dashboard metadata: user={user}, source={source_used}, taxonomy={taxonomy_version}:{groups_hash}")

        # üÜï DEBUG: Log risk_version_info avant retour
        sanitized_dashboard = _clean_for_json(dashboard_data)

        logger.info(
            "üß™ SHADOW V2 DEBUG: dashboard_data['risk_metrics']['risk_version_info'] = %s",
            sanitized_dashboard.get('risk_metrics', {}).get('risk_version_info')
        )

        # üö® CHECK 0: Marqueurs uniques pour prouver l'endpoint atteint
        import time
        sanitized_dashboard["__served_by__"] = "risk_endpoints.py:v2_shadow"
        sanitized_dashboard["__ts__"] = time.time()

        # Cache the result (30 min TTL)
        cache_set(_risk_cache, cache_key, sanitized_dashboard)
        logger.info(f"‚úÖ Risk dashboard computed and cached: {cache_key}")

        return sanitized_dashboard
        
    except Exception as e:
        # ‚úÖ FIX: Differentiate data quality issues (WARNING) from real errors (ERROR)
        error_msg = str(e)
        if "Insufficient data points" in error_msg or "sparse price coverage" in error_msg:
            logger.warning(f"‚ö†Ô∏è Risk dashboard data quality issue: {e}")
        else:
            logger.error(f"‚ùå Erreur dashboard risque: {e}")

        return {
            "success": False,
            "message": f"Erreur lors du calcul dashboard: {error_msg}"
        }

def _generate_centralized_risk_alerts(risk_metrics, correlation_metrics) -> List[Dict[str, Any]]:
    """G√©n√®re les alertes de risque bas√©es sur les m√©triques centralis√©es"""
    alerts = []
    
    # Alert sur Sharpe ratio faible
    if risk_metrics.sharpe_ratio < 0.5:
        alerts.append({
            "level": "high" if risk_metrics.sharpe_ratio < 0 else "medium",
            "type": "performance_alert",
            "message": f"Sharpe ratio faible: {risk_metrics.sharpe_ratio:.2f}",
            "recommendation": "Optimiser la s√©lection d'actifs ou r√©duire la volatilit√©"
        })
    
    # Alert sur drawdown √©lev√©
    if abs(risk_metrics.max_drawdown) > 0.4:
        alerts.append({
            "level": "high",
            "type": "drawdown_alert", 
            "message": f"Drawdown maximum √©lev√©: {risk_metrics.max_drawdown:.1%}",
            "recommendation": "Consid√©rer la diversification ou des strat√©gies de protection"
        })
    
    # Alert sur diversification
    if correlation_metrics.diversification_ratio < 0.6:
        alerts.append({
            "level": "medium",
            "type": "correlation_alert",
            "message": f"Faible diversification: ratio {correlation_metrics.diversification_ratio:.2f}",
            "recommendation": "Ajouter des assets moins corr√©l√©s"
        })
    
    # Alert sur volatilit√© excessive
    if risk_metrics.volatility_annualized > 0.8:
        alerts.append({
            "level": "high",
            "type": "volatility_alert",
            "message": f"Volatilit√© tr√®s √©lev√©e: {risk_metrics.volatility_annualized:.1%}",
            "recommendation": "Augmenter la part de stablecoins ou assets moins volatils"
        })
    
    return alerts

def _get_top_correlations(correlations: Dict[str, Dict[str, float]], top_n: int = 5) -> List[Dict[str, Any]]:
    """Extrait les top N corr√©lations entre assets (excluant self-correlations)"""
    
    correlation_pairs = []
    
    for asset1, corr_dict in correlations.items():
        for asset2, correlation in corr_dict.items():
            if asset1 != asset2 and correlation != 1.0:  # Exclure self-correlation
                # √âviter les doublons (A-B et B-A)
                pair = tuple(sorted([asset1, asset2]))
                correlation_pairs.append({
                    "asset1": pair[0],
                    "asset2": pair[1], 
                    "correlation": correlation
                })
    
    # Supprimer doublons et trier par corr√©lation absolue
    seen = set()
    unique_pairs = []
    for pair in correlation_pairs:
        key = (pair["asset1"], pair["asset2"])
        if key not in seen:
            seen.add(key)
            unique_pairs.append(pair)
    
    unique_pairs.sort(key=lambda x: abs(x["correlation"]), reverse=True)
    return unique_pairs[:top_n]

@router.get("/attribution")
async def get_performance_attribution(
    analysis_days: int = Query(30, ge=7, le=365, description="Analysis period in days")
):
    """
    Calcule l'attribution de performance d√©taill√©e du portfolio
    
    D√©compose la performance totale en:
    - Contributions individuelles par asset
    - Contributions agr√©g√©es par groupe d'assets  
    - Effets d'allocation, s√©lection et interaction
    - Analyse comparative vs benchmark equal-weight
    """
    try:
        start_time = datetime.now()
        
        # Import des balances CoinTracking
        from connectors.cointracking_api import get_current_balances
        
        # R√©cup√©ration des holdings actuels
        balances_response = await get_current_balances()
        if not balances_response or not isinstance(balances_response, dict):
            return {
                "success": False,
                "message": "Erreur lors de la r√©cup√©ration des donn√©es CoinTracking"
            }
        
        balances = balances_response.get('items', [])
        if not balances or len(balances) == 0:
            return {
                "success": False,
                "message": "Aucun holding trouv√© dans le portfolio"
            }
        
        # Calcul de l'attribution de performance
        attribution = await risk_manager.calculate_performance_attribution(
            holdings=balances,
            analysis_days=analysis_days
        )
        
        # Conversion en dictionnaire pour API
        attribution_dict = {
            "success": True,
            "period_analysis": {
                "total_return_pct": attribution.total_return,
                "total_return_usd": attribution.total_return_usd,
                "period_start": attribution.period_start.isoformat(),
                "period_end": attribution.period_end.isoformat(),
                "analysis_days": attribution.analysis_period_days,
                "benchmark_used": attribution.benchmark_used
            },
            "asset_contributions": attribution.asset_contributions,
            "group_contributions": attribution.group_contributions,
            "attribution_effects": {
                "selection_effect": attribution.selection_effect,
                "allocation_effect": attribution.allocation_effect,
                "interaction_effect": attribution.interaction_effect,
                "total_active_return": attribution.selection_effect + attribution.allocation_effect + attribution.interaction_effect
            },
            "top_contributors": sorted(
                attribution.asset_contributions, 
                key=lambda x: x["contribution_pct"], 
                reverse=True
            )[:5],
            "bottom_contributors": sorted(
                attribution.asset_contributions,
                key=lambda x: x["contribution_pct"]
            )[:5]
        }
        
        end_time = datetime.now()
        calculation_time = f"{(end_time - start_time).total_seconds():.2f}s"
        attribution_dict["calculation_time"] = calculation_time
        
        return attribution_dict
        
    except Exception as e:
        logger.error(f"Erreur calcul attribution performance: {e}")
        return {
            "success": False,
            "message": f"Erreur lors du calcul: {str(e)}"
        }

@router.post("/backtest")
async def run_strategy_backtest(
    request: BacktestRequest
):
    """
    Ex√©cute un backtest d'une strat√©gie d'allocation personnalis√©e
    
    Simule la performance d'une strat√©gie sur donn√©es historiques avec:
    - Rebalancing p√©riodique selon la fr√©quence sp√©cifi√©e
    - Co√ªts de transaction r√©alistes
    - Comparaison vs benchmark equal-weight
    - M√©triques de performance compl√®tes
    
    Exemple d'allocations:
    ```json
    {
        "strategy_name": "BTC Heavy",
        "target_allocations": {
            "BTC": 0.5,
            "ETH": 0.3,
            "DeFi": 0.2
        }
    }
    ```
    """
    try:
        start_time = datetime.now()
        
        # Validation des allocations
        total_allocation = sum(request.target_allocations.values())
        if abs(total_allocation - 1.0) > 0.01:
            return {
                "success": False,
                "message": f"Les allocations doivent sommer √† 100% (actuellement {total_allocation:.1%})"
            }
        
        # Ex√©cution du backtest
        backtest_result = await risk_manager.run_strategy_backtest(
            strategy_name=request.strategy_name,
            target_allocations=request.target_allocations,
            backtest_days=request.backtest_days,
            rebalance_frequency_days=request.rebalance_frequency_days,
            transaction_cost_pct=request.transaction_cost_pct
        )
        
        # Conversion en dictionnaire pour API
        result_dict = {
            "success": True,
            "strategy_info": {
                "strategy_name": backtest_result.strategy_name,
                "strategy_description": backtest_result.strategy_description,
                "backtest_period": {
                    "start_date": backtest_result.backtest_start.isoformat(),
                    "end_date": backtest_result.backtest_end.isoformat(),
                    "total_days": backtest_result.backtest_days
                }
            },
            "performance_metrics": {
                "total_return": backtest_result.total_return,
                "annualized_return": backtest_result.annualized_return,
                "volatility": backtest_result.volatility,
                "sharpe_ratio": backtest_result.sharpe_ratio,
                "max_drawdown": backtest_result.max_drawdown,
                "sortino_ratio": backtest_result.sortino_ratio,
                "calmar_ratio": backtest_result.calmar_ratio
            },
            "benchmark_comparison": {
                "benchmark_return": backtest_result.benchmark_return,
                "active_return": backtest_result.active_return,
                "information_ratio": backtest_result.information_ratio,
                "tracking_error": backtest_result.tracking_error
            },
            "risk_metrics": {
                "var_95": backtest_result.var_95,
                "downside_deviation": backtest_result.downside_deviation
            },
            "trading_statistics": {
                "num_rebalances": backtest_result.num_rebalances,
                "avg_turnover": backtest_result.avg_turnover,
                "total_costs": backtest_result.total_costs,
                "rebalance_frequency_days": request.rebalance_frequency_days,
                "transaction_cost_pct": request.transaction_cost_pct
            },
            "performance_chart": {
                "dates": [d.isoformat() for d in backtest_result.dates],
                "portfolio_values": backtest_result.portfolio_values,
                "benchmark_values": backtest_result.benchmark_values,
                "rebalancing_dates": [d.isoformat() for d in backtest_result.rebalancing_dates]
            }
        }
        
        end_time = datetime.now()
        calculation_time = f"{(end_time - start_time).total_seconds():.2f}s"
        result_dict["calculation_time"] = calculation_time
        
        return result_dict
        
    except Exception as e:
        logger.error(f"Erreur backtest: {e}")
        return {
            "success": False,
            "message": f"Erreur lors du backtest: {str(e)}"
        }

@router.get("/alerts")
async def get_risk_alerts(
    severity_filter: Optional[str] = Query(None, description="Filter by severity (info/low/medium/high/critical)")
):
    """
    R√©cup√®re les alertes de risque actives
    
    Retourne toutes les alertes intelligentes g√©n√©r√©es par le syst√®me,
    incluant les d√©passements de seuils, probl√®mes de performance,
    concentration excessive, etc.
    """
    try:
        # Import des balances CoinTracking
        from connectors.cointracking_api import get_current_balances
        
        # R√©cup√©ration des holdings actuels
        balances_response = await get_current_balances()
        if not balances_response or not isinstance(balances_response, dict):
            return {
                "success": False,
                "message": "Erreur lors de la r√©cup√©ration des donn√©es CoinTracking"
            }
        
        balances = balances_response.get('items', [])
        if not balances or len(balances) == 0:
            return {
                "success": False,
                "message": "Aucun holding trouv√© dans le portfolio"
            }
        
        # G√©n√©ration des alertes intelligentes
        alerts = await risk_manager.generate_intelligent_alerts(holdings=balances)
        
        # Filtrage par s√©v√©rit√© si demand√©
        severity_enum = None
        if severity_filter:
            try:
                severity_enum = AlertSeverity(severity_filter.lower())
            except ValueError:
                return {
                    "success": False,
                    "message": f"S√©v√©rit√© invalide. Options: {[s.value for s in AlertSeverity]}"
                }
        
        # R√©cup√©ration des alertes actives
        active_alerts = risk_manager.alert_system.get_active_alerts(severity_enum)
        
        # Conversion en dictionnaire pour API
        alerts_data = []
        for alert in active_alerts:
            alerts_data.append({
                "id": alert.id,
                "severity": alert.severity.value,
                "category": alert.category.value,
                "title": alert.title,
                "message": alert.message,
                "recommendation": alert.recommendation,
                "current_value": alert.current_value,
                "threshold_value": alert.threshold_value,
                "affected_assets": alert.affected_assets,
                "created_at": alert.created_at.isoformat(),
                "trigger_count": alert.trigger_count,
                "is_active": alert.is_active
            })
        
        return {
            "success": True,
            "alerts": alerts_data,
            "summary": {
                "total_alerts": len(alerts_data),
                "critical": len([a for a in alerts_data if a["severity"] == "critical"]),
                "high": len([a for a in alerts_data if a["severity"] == "high"]),
                "medium": len([a for a in alerts_data if a["severity"] == "medium"]),
                "low": len([a for a in alerts_data if a["severity"] == "low"]),
                "info": len([a for a in alerts_data if a["severity"] == "info"])
            }
        }
        
    except Exception as e:
        logger.error(f"Erreur r√©cup√©ration alertes: {e}")
        return {
            "success": False,
            "message": f"Erreur lors de la r√©cup√©ration: {str(e)}"
        }

# REMOVED: Duplicate alert resolution endpoint - use /api/alerts/resolve/{alert_id} instead
# Alert management should be centralized in alerts_endpoints.py

@router.get("/alerts/history")
async def get_alerts_history(
    limit: int = Query(50, ge=1, le=500, description="Number of alerts to return")
):
    """
    R√©cup√®re l'historique des alertes
    
    Retourne les alertes pass√©es (r√©solues et expir√©es) pour analyse historique.
    """
    try:
        # R√©cup√©ration de l'historique
        history = risk_manager.alert_system.alert_history[-limit:]
        
        # Conversion en dictionnaire pour API
        history_data = []
        for alert in reversed(history):  # Plus r√©centes en premier
            history_data.append({
                "id": alert.id,
                "severity": alert.severity.value,
                "category": alert.category.value,
                "title": alert.title,
                "message": alert.message,
                "current_value": alert.current_value,
                "threshold_value": alert.threshold_value,
                "affected_assets": alert.affected_assets,
                "created_at": alert.created_at.isoformat(),
                "expires_at": alert.expires_at.isoformat() if alert.expires_at else None,
                "is_active": alert.is_active,
                "resolution_note": alert.resolution_note,
                "trigger_count": alert.trigger_count
            })
        
        return {
            "success": True,
            "history": history_data,
            "total_history_size": len(risk_manager.alert_system.alert_history),
            "returned_count": len(history_data)
        }
        
    except Exception as e:
        logger.error(f"Erreur historique alertes: {e}")
        return {
            "success": False,
            "message": f"Erreur lors de la r√©cup√©ration: {str(e)}"
        }

def _generate_risk_alerts(risk_metrics: RiskMetrics, correlation_matrix: CorrelationMatrix) -> List[Dict[str, Any]]:
    """G√©n√®re des alertes de risque intelligentes"""
    
    alerts = []
    
    # Alert VaR √©lev√©
    if risk_metrics.var_95_1d > 0.15:  # VaR > 15%
        alerts.append({
            "level": "high",
            "type": "var_alert",
            "message": f"VaR 95% √©lev√©: {risk_metrics.var_95_1d:.1%} (seuil: 15%)",
            "recommendation": "Consid√©rer r√©duire l'exposition aux assets volatils"
        })
    
    # Alert volatilit√© excessive
    if risk_metrics.volatility_annualized > 0.80:  # Volatilit√© > 80%
        alerts.append({
            "level": "medium",
            "type": "volatility_alert", 
            "message": f"Volatilit√© √©lev√©e: {risk_metrics.volatility_annualized:.1%}",
            "recommendation": "R√©√©quilibrer vers des assets moins volatils"
        })
    
    # Alert drawdown actuel
    if risk_metrics.current_drawdown > 0.20:  # Drawdown > 20%
        alerts.append({
            "level": "high",
            "type": "drawdown_alert",
            "message": f"Drawdown actuel important: {risk_metrics.current_drawdown:.1%}",
            "recommendation": "Surveiller de pr√®s et consid√©rer stop-loss"
        })
    
    # Alert corr√©lation excessive (manque de diversification)
    # Harmonis√© avec l'UI: bon ‚â•0.7, limit√© 0.4‚Äì0.7, faible <0.4
    try:
        dr = float(correlation_matrix.diversification_ratio)
        if dr < 0.4:
            alerts.append({
                "level": "high",
                "type": "correlation_alert",
                "message": f"Tr√®s faible diversification: ratio {dr:.2f}",
                "recommendation": "R√©duire l'exposition aux actifs fortement corr√©l√©s; ajouter des actifs d√©corr√©l√©s"
            })
        elif dr < 0.7:
            alerts.append({
                "level": "medium",
                "type": "correlation_alert",
                "message": f"Faible diversification: ratio {dr:.2f}",
                "recommendation": "Ajouter des assets moins corr√©l√©s"
            })
    except Exception as e:
        logger.warning(f"Failed to calculate diversification alert: {e}")
    
    # Alert Sharpe ratio n√©gatif
    if risk_metrics.sharpe_ratio < 0:
        alerts.append({
            "level": "medium",
            "type": "performance_alert",
            "message": f"Sharpe ratio n√©gatif: {risk_metrics.sharpe_ratio:.2f}",
            "recommendation": "Revoir la strat√©gie d'allocation"
        })

    return alerts


# ===== NEW ENDPOINTS: Stress Testing & Monte Carlo (Dec 2025) =====

@router.get("/stress-scenarios")
async def get_stress_scenarios():
    """
    Liste tous les sc√©narios de stress test disponibles

    Returns:
        Liste des sc√©narios avec m√©tadonn√©es (impact, probabilit√©, dur√©e)
    """
    try:
        from services.risk.stress_testing import get_available_scenarios

        scenarios = get_available_scenarios()

        return {
            "success": True,
            "scenarios": scenarios,
            "count": len(scenarios)
        }

    except Exception as e:
        logger.error(f"Failed to get stress scenarios: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/stress-test-portfolio")
async def run_stress_test_portfolio(
    scenario_id: str = Query(..., description="Scenario ID (crisis_2008, covid_2020, etc.)"),
    user: str = Depends(get_required_user)
):
    """
    Ex√©cute un stress test r√©el sur le portfolio avec un sc√©nario donn√©

    Sc√©narios disponibles:
    - crisis_2008: Crise financi√®re 2008
    - covid_2020: Crash COVID Mars 2020
    - china_ban: Interdiction crypto Chine
    - tether_collapse: Effondrement Tether
    - fed_emergency: Hausse taux Fed d'urgence
    - exchange_hack: Hack exchange majeur

    Returns:
        Impact d√©taill√© sur le portfolio (pertes par groupe, pires/meilleurs performers)
    """
    try:
        from services.risk.stress_testing import calculate_stress_test
        from api.unified_data import get_unified_filtered_balances

        # R√©cup√©rer portfolio actuel
        balances_response = await get_unified_filtered_balances(
            source="cointracking",
            min_usd=1.0,
            user_id=user
        )

        balances = balances_response.get('items', [])

        if not balances or len(balances) == 0:
            return {
                "success": False,
                "message": "No holdings found in portfolio"
            }

        # Ex√©cuter stress test
        result = await calculate_stress_test(
            holdings=balances,
            scenario_id=scenario_id,
            user_id=user
        )

        # Convertir en dict pour JSON
        return {
            "success": True,
            "result": {
                "scenario_id": result.scenario_id,
                "scenario_name": result.scenario_name,
                "scenario_description": result.scenario_description,
                "portfolio_impact": {
                    "loss_pct": result.portfolio_loss_pct,
                    "loss_usd": result.portfolio_loss_usd,
                    "value_before": result.portfolio_value_before,
                    "value_after": result.portfolio_value_after
                },
                "group_impacts": result.group_impacts,
                "worst_groups": result.worst_groups,
                "best_groups": result.best_groups,
                "metadata": {
                    "probability_10y": result.probability_10y,
                    "duration_estimate": result.duration_estimate,
                    "timestamp": result.timestamp.isoformat()
                }
            }
        }

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Stress test failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/monte-carlo")
async def run_monte_carlo(
    num_simulations: int = Query(10000, ge=1000, le=50000, description="Number of simulations"),
    horizon_days: int = Query(30, ge=1, le=365, description="Forecast horizon in days"),
    confidence_level: float = Query(0.95, ge=0.90, le=0.99, description="Confidence level for VaR"),
    price_history_days: int = Query(365, ge=90, le=730, description="Days of price history for distributions"),
    # ‚úÖ FIX: Accept extra params from frontend (global-config.js adds these automatically)
    source: str = Query("cointracking", description="Data source (ignored, uses unified data)"),
    pricing: str = Query("auto", description="Pricing source (ignored)"),
    min_usd: float = Query(1.0, description="Min USD threshold (ignored)"),
    user: str = Depends(get_required_user)
):
    """
    Ex√©cute une simulation Monte Carlo sur le portfolio

    G√©n√®re N simulations bas√©es sur les distributions historiques r√©elles
    de rendements (pr√©serve corr√©lations entre assets)

    Returns:
        - Statistiques distribution (moyenne, m√©diane, √©cart-type)
        - Sc√©narios extr√™mes (meilleur/pire cas)
        - Probabilit√©s de pertes (>5%, >10%, >20%, >30%)
        - VaR/CVaR Monte Carlo (95%, 99%)
        - Distribution compl√®te (percentiles)
    """
    try:
        from services.risk.monte_carlo import run_monte_carlo_simulation
        from api.unified_data import get_unified_filtered_balances

        # R√©cup√©rer portfolio actuel (use unified data, ignore source param)
        balances_response = await get_unified_filtered_balances(
            source="cointracking",  # Force cointracking for stability
            min_usd=1.0,
            user_id=user
        )

        balances = balances_response.get('items', [])

        if not balances or len(balances) == 0:
            return {
                "success": False,
                "message": "No holdings found in portfolio"
            }

        # Ex√©cuter Monte Carlo
        result = await run_monte_carlo_simulation(
            holdings=balances,
            num_simulations=num_simulations,
            horizon_days=horizon_days,
            confidence_level=confidence_level,
            price_history_days=price_history_days,
            user_id=user
        )

        # Convertir en dict pour JSON
        return {
            "success": True,
            "result": {
                "simulation_params": {
                    "num_simulations": result.num_simulations,
                    "horizon_days": result.horizon_days,
                    "confidence_level": result.confidence_level,
                    "portfolio_value": result.portfolio_value,
                    "num_assets": result.num_assets
                },
                "statistics": {
                    "mean_return_pct": result.mean_return_pct,
                    "median_return_pct": result.median_return_pct,
                    "std_return_pct": result.std_return_pct
                },
                "scenarios": {
                    "worst_case_pct": result.worst_case_pct,
                    "best_case_pct": result.best_case_pct,
                    "percentile_5_pct": result.percentile_5_pct,
                    "percentile_95_pct": result.percentile_95_pct
                },
                "loss_probabilities": {
                    "prob_loss_any": result.prob_loss_any,
                    "prob_loss_5": result.prob_loss_5,
                    "prob_loss_10": result.prob_loss_10,
                    "prob_loss_20": result.prob_loss_20,
                    "prob_loss_30": result.prob_loss_30
                },
                "risk_metrics": {
                    "var_95_pct": result.var_95_pct,
                    "cvar_95_pct": result.cvar_95_pct,
                    "var_99_pct": result.var_99_pct,
                    "cvar_99_pct": result.cvar_99_pct
                },
                "distribution_percentiles": result.distribution_percentiles,
                "timestamp": result.timestamp.isoformat()
            }
        }

    except ValueError as e:
        logger.error(f"Monte Carlo validation error: {e}")
        raise HTTPException(status_code=400, detail=f"Validation error: {str(e)}")
    except np.linalg.LinAlgError as e:
        logger.error(f"Monte Carlo numerical error (SVD): {e}")
        raise HTTPException(status_code=500, detail=f"Numerical error in covariance calculation: {str(e)}")
    except Exception as e:
        logger.error(f"Monte Carlo simulation failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Simulation failed: {str(e)}")

