"""
Advanced Portfolio Risk Management - Syst√®me de gestion des risques de niveau institutionnel

Ce module impl√©mente des m√©triques de risque sophistiqu√©es :
- VaR (Value at Risk) et CVaR (Conditional VaR) 
- Matrice de corr√©lation temps r√©el entre assets
- Stress testing avec sc√©narios crypto historiques
- M√©triques avanc√©es : Sortino, Calmar, Maximum Drawdown, Ulcer Index
- Attribution de performance par asset/strat√©gie
- Monitoring temps r√©el avec alertes intelligentes
"""

from __future__ import annotations
import asyncio
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import math
from scipy import stats
from collections import deque

from services.taxonomy import Taxonomy
from services.pricing import get_prices_usd
from services.portfolio import portfolio_analytics

logger = logging.getLogger(__name__)

class RiskLevel(Enum):
    """Niveaux de risque pour alertes"""
    VERY_LOW = "very_low"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"
    CRITICAL = "critical"

class StressScenario(Enum):
    """Sc√©narios de stress test historiques crypto"""
    BEAR_MARKET_2018 = "bear_2018"          # Crash 2018: BTC -84%, Altcoins -95%
    COVID_CRASH_2020 = "covid_2020"         # Mars 2020: BTC -50% en 2 semaines  
    LUNA_COLLAPSE_2022 = "luna_2022"        # Mai 2022: Terra Luna collapse
    FTX_COLLAPSE_2022 = "ftx_2022"          # Nov 2022: FTX bankruptcy
    CUSTOM_SCENARIO = "custom"               # Sc√©nario personnalis√©

@dataclass
class RiskMetrics:
    """M√©triques de risque compl√®tes pour un portfolio"""
    
    # VaR/CVaR 
    var_95_1d: float = 0.0           # VaR 95% 1 jour
    var_99_1d: float = 0.0           # VaR 99% 1 jour  
    cvar_95_1d: float = 0.0          # CVaR 95% 1 jour
    cvar_99_1d: float = 0.0          # CVaR 99% 1 jour
    
    # M√©triques classiques
    volatility_annualized: float = 0.0      # Volatilit√© annualis√©e
    sharpe_ratio: float = 0.0               # Sharpe ratio
    sortino_ratio: float = 0.0              # Sortino ratio (downside deviation)
    calmar_ratio: float = 0.0               # Calmar ratio (return/max drawdown)
    
    # Drawdown analysis
    max_drawdown: float = 0.0               # Maximum drawdown
    max_drawdown_duration_days: int = 0     # Dur√©e max drawdown
    current_drawdown: float = 0.0           # Drawdown actuel
    ulcer_index: float = 0.0                # Ulcer index (drawdown pain)
    
    # Distribution analysis
    skewness: float = 0.0                   # Asym√©trie des returns
    kurtosis: float = 0.0                   # Aplatissement (fat tails)
    
    # Risk level assessment
    overall_risk_level: RiskLevel = RiskLevel.MEDIUM
    risk_score: float = 0.0                 # Score 0-100
    
    # Metadata
    calculation_date: datetime = field(default_factory=datetime.now)
    data_points: int = 0                    # Nombre de points de donn√©es
    confidence_level: float = 0.0           # Niveau de confiance des calculs

@dataclass  
class CorrelationMatrix:
    """Matrice de corr√©lation entre assets"""
    correlations: Dict[str, Dict[str, float]] = field(default_factory=dict)
    eigen_values: List[float] = field(default_factory=list)
    eigen_vectors: List[List[float]] = field(default_factory=list) 
    principal_components: Dict[str, float] = field(default_factory=dict)
    diversification_ratio: float = 0.0      # Ratio de diversification
    effective_assets: float = 0.0           # Nombre effectif d'assets ind√©pendants
    last_updated: datetime = field(default_factory=datetime.now)

@dataclass
class StressTestResult:
    """R√©sultat d'un stress test"""
    scenario_name: str
    scenario_description: str
    portfolio_loss_pct: float               # Perte portfolio en %
    portfolio_loss_usd: float               # Perte portfolio en USD
    worst_performing_assets: List[Dict[str, Any]]  # Top 3 pires performances
    best_performing_assets: List[Dict[str, Any]]   # Top 3 meilleures performances
    var_breach: bool                        # VaR d√©pass√© ou non
    recovery_time_estimate_days: int        # Temps de r√©cup√©ration estim√©
    risk_contribution: Dict[str, float]     # Contribution au risque par asset

@dataclass
class PerformanceAttribution:
    """Attribution de performance d√©taill√©e du portfolio"""
    total_return: float                     # Return total du portfolio
    total_return_usd: float                 # Return en USD
    
    # Attribution par asset individuel
    asset_contributions: List[Dict[str, Any]] = field(default_factory=list)
    
    # Attribution par groupe d'assets
    group_contributions: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    
    # M√©triques d'attribution
    selection_effect: float = 0.0          # Effet de s√©lection d'assets
    allocation_effect: float = 0.0         # Effet d'allocation entre groupes
    interaction_effect: float = 0.0        # Effet d'interaction
    
    # Analyse temporelle
    period_start: datetime = field(default_factory=datetime.now)
    period_end: datetime = field(default_factory=datetime.now) 
    analysis_period_days: int = 0
    
    # M√©tadonn√©es
    calculation_date: datetime = field(default_factory=datetime.now)
    benchmark_used: Optional[str] = None    # Benchmark utilis√© pour comparaison

@dataclass
class BacktestResult:
    """R√©sultats d'un backtest de strat√©gie"""
    strategy_name: str
    strategy_description: str
    
    # Performance globale
    total_return: float                     # Return total de la strat√©gie
    annualized_return: float               # Return annualis√©
    volatility: float                      # Volatilit√© annualis√©e
    sharpe_ratio: float                    # Sharpe ratio
    max_drawdown: float                    # Maximum drawdown
    
    # Comparaison vs benchmark
    benchmark_return: float                # Return du benchmark
    active_return: float                   # Return actif (strat√©gie - benchmark)
    information_ratio: float               # Information ratio
    tracking_error: float                  # Tracking error vs benchmark
    
    # M√©triques de risque
    var_95: float                          # VaR 95%
    downside_deviation: float              # D√©viation downside
    sortino_ratio: float                   # Sortino ratio
    calmar_ratio: float                    # Calmar ratio
    
    # Statistiques de trading
    num_rebalances: int                    # Nombre de rebalancing
    avg_turnover: float                    # Turnover moyen par rebalancing
    total_costs: float                     # Co√ªts totaux de transaction
    
    # Historique d√©taill√©
    portfolio_values: List[float] = field(default_factory=list)  # Valeur portfolio dans le temps
    benchmark_values: List[float] = field(default_factory=list)  # Valeur benchmark dans le temps
    dates: List[datetime] = field(default_factory=list)          # Dates correspondantes
    rebalancing_dates: List[datetime] = field(default_factory=list)  # Dates de rebalancing
    
    # M√©tadonn√©es
    backtest_start: datetime = field(default_factory=datetime.now)
    backtest_end: datetime = field(default_factory=datetime.now)
    backtest_days: int = 0

class AlertSeverity(Enum):
    """Niveaux de s√©v√©rit√© des alertes"""
    INFO = "info"
    LOW = "low" 
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class AlertCategory(Enum):
    """Cat√©gories d'alertes"""
    RISK_THRESHOLD = "risk_threshold"           # D√©passement de seuils de risque
    PERFORMANCE = "performance"                 # Alertes de performance
    CORRELATION = "correlation"                 # Alertes de corr√©lation
    CONCENTRATION = "concentration"             # Alertes de concentration
    MARKET_STRESS = "market_stress"            # Alertes de stress march√©
    REBALANCING = "rebalancing"                # Alertes de rebalancing
    DATA_QUALITY = "data_quality"              # Alertes qualit√© des donn√©es

@dataclass
class RiskAlert:
    """Alerte de risque intelligente"""
    id: str                                    # ID unique de l'alerte
    severity: AlertSeverity                    # Niveau de s√©v√©rit√©
    category: AlertCategory                    # Cat√©gorie
    title: str                                 # Titre court
    message: str                               # Message descriptif
    recommendation: str                        # Recommandation d'action
    
    # Donn√©es contextuelles
    current_value: float                       # Valeur actuelle du m√©trique
    threshold_value: float                     # Seuil d√©pass√©
    affected_assets: List[str] = field(default_factory=list)  # Assets concern√©s
    
    # M√©tadonn√©es
    created_at: datetime = field(default_factory=datetime.now)
    expires_at: Optional[datetime] = None      # Expiration de l'alerte
    is_active: bool = True                     # Alerte active ou r√©solue
    resolution_note: Optional[str] = None      # Note de r√©solution
    
    # Historique
    first_triggered: datetime = field(default_factory=datetime.now)
    trigger_count: int = 1                     # Nombre de d√©clenchements

@dataclass 
class AlertSystem:
    """Syst√®me d'alertes intelligent avec historique et r√®gles configurables"""
    
    # Configuration des seuils
    thresholds: Dict[str, Dict[str, float]] = field(default_factory=dict)
    
    # Historique des alertes
    alert_history: List[RiskAlert] = field(default_factory=list)
    active_alerts: Dict[str, RiskAlert] = field(default_factory=dict)
    
    # Param√®tres syst√®me
    max_alert_history: int = 1000
    alert_cooldown_hours: int = 24
    
    def __post_init__(self):
        """Initialise les seuils par d√©faut"""
        self.thresholds = self._get_default_thresholds()
    
    def _get_default_thresholds(self) -> Dict[str, Dict[str, float]]:
        """D√©finit les seuils par d√©faut pour chaque type d'alerte"""
        return {
            "var_95": {
                "medium": 0.10,    # VaR 95% > 10%
                "high": 0.15,      # VaR 95% > 15% 
                "critical": 0.25   # VaR 95% > 25%
            },
            "volatility": {
                "medium": 0.60,    # Volatilit√© > 60%
                "high": 0.80,      # Volatilit√© > 80%
                "critical": 1.20   # Volatilit√© > 120%
            },
            "max_drawdown": {
                "medium": 0.20,    # Max DD > 20%
                "high": 0.35,      # Max DD > 35%
                "critical": 0.50   # Max DD > 50%
            },
            "current_drawdown": {
                "medium": 0.15,    # Drawdown actuel > 15%
                "high": 0.25,      # Drawdown actuel > 25%
                "critical": 0.40   # Drawdown actuel > 40%
            },
            "diversification_ratio": {
                "medium": 0.7,     # Ratio < 0.7 (faible)
                "high": 0.4        # Ratio < 0.4 (tr√®s faible)
            },
            "concentration": {
                "medium": 0.60,    # Plus de 60% dans un asset
                "high": 0.75,      # Plus de 75% dans un asset
                "critical": 0.90   # Plus de 90% dans un asset
            },
            "sharpe_ratio": {
                "medium": 0.0,     # Sharpe < 0 (n√©gatif)
                "high": -0.5       # Sharpe < -0.5 (tr√®s n√©gatif)
            }
        }
    
    def generate_alert(
        self,
        alert_type: str,
        severity: AlertSeverity,
        category: AlertCategory,
        title: str,
        message: str,
        recommendation: str,
        current_value: float,
        threshold_value: float,
        affected_assets: List[str] = None
    ) -> RiskAlert:
        """G√©n√®re une nouvelle alerte"""
        
        alert_id = f"{category.value}_{alert_type}_{int(datetime.now().timestamp())}"
        
        # V√©rifier si alerte existe d√©j√† (cooldown)
        existing_alert = self._find_existing_alert(alert_type, category)
        if existing_alert:
            # Mettre √† jour l'alerte existante
            existing_alert.trigger_count += 1
            existing_alert.current_value = current_value
            existing_alert.created_at = datetime.now()
            return existing_alert
        
        # Cr√©er nouvelle alerte
        alert = RiskAlert(
            id=alert_id,
            severity=severity,
            category=category,
            title=title,
            message=message,
            recommendation=recommendation,
            current_value=current_value,
            threshold_value=threshold_value,
            affected_assets=affected_assets or []
        )
        
        # Ajouter √† l'historique et aux alertes actives
        self.alert_history.append(alert)
        self.active_alerts[alert_id] = alert
        
        # Nettoyer l'historique si n√©cessaire
        if len(self.alert_history) > self.max_alert_history:
            self.alert_history = self.alert_history[-self.max_alert_history:]
        
        return alert
    
    def _find_existing_alert(self, alert_type: str, category: AlertCategory) -> Optional[RiskAlert]:
        """Trouve une alerte existante du m√™me type dans la p√©riode de cooldown"""
        
        cooldown_threshold = datetime.now() - timedelta(hours=self.alert_cooldown_hours)
        
        for alert in self.active_alerts.values():
            if (alert.category == category and 
                alert_type in alert.id and 
                alert.created_at > cooldown_threshold and
                alert.is_active):
                return alert
        
        return None
    
    def resolve_alert(self, alert_id: str, resolution_note: str = "") -> bool:
        """R√©sout une alerte active"""
        
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.is_active = False
            alert.resolution_note = resolution_note
            alert.expires_at = datetime.now()
            
            # Retirer des alertes actives
            del self.active_alerts[alert_id]
            return True
        
        return False
    
    def get_active_alerts(self, severity_filter: Optional[AlertSeverity] = None) -> List[RiskAlert]:
        """R√©cup√®re les alertes actives, optionnellement filtr√©es par s√©v√©rit√©"""
        
        alerts = list(self.active_alerts.values())
        
        if severity_filter:
            alerts = [a for a in alerts if a.severity == severity_filter]
        
        # Trier par s√©v√©rit√© et date
        severity_order = {
            AlertSeverity.CRITICAL: 5,
            AlertSeverity.HIGH: 4,
            AlertSeverity.MEDIUM: 3,
            AlertSeverity.LOW: 2,
            AlertSeverity.INFO: 1
        }
        
        alerts.sort(key=lambda x: (severity_order[x.severity], x.created_at), reverse=True)
        
        return alerts
    
    def cleanup_expired_alerts(self):
        """Nettoie les alertes expir√©es"""
        
        now = datetime.now()
        expired_ids = []
        
        for alert_id, alert in self.active_alerts.items():
            if alert.expires_at and alert.expires_at < now:
                expired_ids.append(alert_id)
        
        for alert_id in expired_ids:
            del self.active_alerts[alert_id]

class AdvancedRiskManager:
    """Gestionnaire de risques avanc√© avec m√©triques institutionnelles"""
    
    def __init__(self):
        # Cache de donn√©es historiques simul√©es (en production: vraies donn√©es)
        self.price_history_cache: Dict[str, deque] = {}
        self.max_history_days = 365
        
        # Param√®tres des mod√®les
        self.var_confidence_levels = [0.95, 0.99]
        self.risk_free_rate = 0.02  # Taux sans risque annuel (2%)
        
        # Sc√©narios de stress pr√©d√©finis
        self.stress_scenarios = self._build_stress_scenarios()
        
        # Cache des r√©sultats
        self.risk_metrics_cache: Dict[str, RiskMetrics] = {}
        self.correlation_cache: Optional[CorrelationMatrix] = None
        self.cache_ttl = timedelta(hours=1)
        
        # Alertes et seuils
        self.risk_thresholds = {
            RiskLevel.VERY_LOW: {"var_95": 0.02, "volatility": 0.1},
            RiskLevel.LOW: {"var_95": 0.05, "volatility": 0.2},
            RiskLevel.MEDIUM: {"var_95": 0.10, "volatility": 0.4},
            RiskLevel.HIGH: {"var_95": 0.15, "volatility": 0.6},
            RiskLevel.VERY_HIGH: {"var_95": 0.25, "volatility": 1.0},
            RiskLevel.CRITICAL: {"var_95": 0.40, "volatility": 1.5}
        }
        
        # Syst√®me d'alertes intelligent
        self.alert_system = AlertSystem()
    
    def _build_stress_scenarios(self) -> Dict[StressScenario, Dict[str, Any]]:
        """Construit les sc√©narios de stress test bas√©s sur l'historique crypto"""
        
        return {
            StressScenario.BEAR_MARKET_2018: {
                "name": "Bear Market 2018",
                "description": "Crash crypto 2018: BTC -84%, ETH -94%, Altcoins -95%",
                "asset_shocks": {
                    "BTC": -0.84,
                    "ETH": -0.94, 
                    "Stablecoins": 0.0,
                    "L1/L0 majors": -0.95,
                    "L2/Scaling": -0.98,  # N'existaient pas encore
                    "DeFi": -0.98,        # N'existait pas encore
                    "AI/Data": -0.98,
                    "Gaming/NFT": -0.98,
                    "Memecoins": -0.99,
                    "Others": -0.96
                },
                "correlation_increase": 0.3,  # Corr√©lations augmentent en crise
                "duration_days": 365,
                "volatility_multiplier": 3.0
            },
            
            StressScenario.COVID_CRASH_2020: {
                "name": "COVID Crash Mars 2020", 
                "description": "Liquidation massive: BTC -50% en 2 semaines, tout corr√©l√©",
                "asset_shocks": {
                    "BTC": -0.50,
                    "ETH": -0.60,
                    "Stablecoins": 0.05,  # Flight to safety
                    "L1/L0 majors": -0.65,
                    "L2/Scaling": -0.70,
                    "DeFi": -0.80,        # DeFi panic
                    "AI/Data": -0.75,
                    "Gaming/NFT": -0.80,
                    "Memecoins": -0.85,
                    "Others": -0.70
                },
                "correlation_increase": 0.5,  # Tout devient corr√©l√©
                "duration_days": 14,
                "volatility_multiplier": 5.0
            },
            
            StressScenario.LUNA_COLLAPSE_2022: {
                "name": "Terra Luna Collapse Mai 2022",
                "description": "Effondrement UST/LUNA, contagion DeFi et stablecoins",
                "asset_shocks": {
                    "BTC": -0.30,
                    "ETH": -0.35,
                    "Stablecoins": -0.05,  # Doute sur stablecoins
                    "L1/L0 majors": -0.45,
                    "L2/Scaling": -0.50,
                    "DeFi": -0.70,        # Contagion DeFi forte
                    "AI/Data": -0.40,
                    "Gaming/NFT": -0.55,
                    "Memecoins": -0.60,
                    "Others": -0.50
                },
                "correlation_increase": 0.2,
                "duration_days": 30,
                "volatility_multiplier": 2.5
            },
            
            StressScenario.FTX_COLLAPSE_2022: {
                "name": "FTX Collapse Novembre 2022",
                "description": "Bankruptcy FTX, crise de confiance, liquidit√© gel√©e",
                "asset_shocks": {
                    "BTC": -0.25,
                    "ETH": -0.30,
                    "Stablecoins": 0.02,  # Flight to quality stables
                    "L1/L0 majors": -0.35,
                    "L2/Scaling": -0.40,
                    "DeFi": -0.50,        # Liquidit√© DeFi touch√©e
                    "AI/Data": -0.30,
                    "Gaming/NFT": -0.45,
                    "Memecoins": -0.55,
                    "Others": -0.40
                },
                "correlation_increase": 0.25,
                "duration_days": 45,
                "volatility_multiplier": 2.0
            }
        }
    
    async def calculate_portfolio_risk_metrics(
        self, 
        holdings: List[Dict[str, Any]],
        price_history_days: int = 30
    ) -> RiskMetrics:
        """
        Calcule les m√©triques de risque compl√®tes pour un portfolio
        
        Args:
            holdings: Holdings actuels du portfolio
            price_history_days: Nombre de jours d'historique pour calculs
            
        Returns:
            RiskMetrics avec tous les indicateurs de risque
        """
        try:
            logger.info(f"Calcul m√©triques risque pour {len(holdings)} holdings")
            
            # 1. Pr√©paration des donn√©es
            portfolio_value = sum(h.get("value_usd", 0) for h in holdings)
            if portfolio_value <= 0:
                return RiskMetrics()
            
            # 2. G√©n√©ration historique des prix (simulation - en production: vraies donn√©es)
            returns_data = await self._generate_historical_returns(holdings, price_history_days)
            
            if len(returns_data) < 10:  # Minimum de donn√©es requis
                logger.warning("Pas assez de donn√©es historiques pour calculs fiables")
                return RiskMetrics(confidence_level=0.0)
            
            # 3. Calcul des returns du portfolio
            portfolio_returns = self._calculate_portfolio_returns(holdings, returns_data)
            
            # 4. Fen√™tres intelligentes par m√©trique (cycle-aware)
            def tail(seq, n):
                n_eff = min(len(seq), max(0, int(n)))
                return seq[-n_eff:] if n_eff > 0 else []

            windows = {
                "var": 30,
                "cvar": 60,
                "vol": 45,
                "sharpe": 90,
                "sortino": 120,
                "dd": 180,
                "calmar": 365,
            }

            # VaR/CVaR
            var_returns = tail(portfolio_returns, windows["var"])
            cvar_returns = tail(portfolio_returns, windows["cvar"])

            var_metrics = self._calculate_var_cvar(var_returns)
            if len(cvar_returns) >= 10:
                cvar_only = self._calculate_var_cvar(cvar_returns)
                var_metrics["cvar_95"] = cvar_only.get("cvar_95", var_metrics["cvar_95"]) 
                var_metrics["cvar_99"] = cvar_only.get("cvar_99", var_metrics["cvar_99"]) 

            # Performance ajust√©e au risque
            vol_metrics = self._calculate_risk_adjusted_metrics(tail(portfolio_returns, windows["vol"]))
            sharpe_metrics = self._calculate_risk_adjusted_metrics(tail(portfolio_returns, windows["sharpe"]))
            sortino_metrics = self._calculate_risk_adjusted_metrics(tail(portfolio_returns, windows["sortino"]))
            calmar_metrics = self._calculate_risk_adjusted_metrics(tail(portfolio_returns, windows["calmar"]))

            perf_metrics = {
                "volatility": vol_metrics.get("volatility", 0.0),
                "sharpe": sharpe_metrics.get("sharpe", 0.0),
                "sortino": sortino_metrics.get("sortino", 0.0),
                "calmar": calmar_metrics.get("calmar", 0.0),
            }

            # Drawdowns
            drawdown_metrics = self._calculate_drawdown_metrics(tail(portfolio_returns, windows["dd"]))

            # Distribution (utiliser fen√™tre sharpe par d√©faut)
            distribution_metrics = self._calculate_distribution_metrics(tail(portfolio_returns, windows["sharpe"]))
            
            # 8. √âvaluation du niveau de risque global
            risk_assessment = self._assess_overall_risk_level(var_metrics, perf_metrics, drawdown_metrics)
            
            # 9. Construction du r√©sultat final
            metrics = RiskMetrics(
                # VaR/CVaR
                var_95_1d=var_metrics["var_95"],
                var_99_1d=var_metrics["var_99"],
                cvar_95_1d=var_metrics["cvar_95"],
                cvar_99_1d=var_metrics["cvar_99"],
                
                # Performance ajust√©e au risque
                volatility_annualized=perf_metrics["volatility"],
                sharpe_ratio=perf_metrics["sharpe"],
                sortino_ratio=perf_metrics["sortino"],
                calmar_ratio=perf_metrics["calmar"],
                
                # Drawdowns
                max_drawdown=drawdown_metrics["max_drawdown"],
                max_drawdown_duration_days=drawdown_metrics["max_duration"],
                current_drawdown=drawdown_metrics["current_drawdown"],
                ulcer_index=drawdown_metrics["ulcer_index"],
                
                # Distribution
                skewness=distribution_metrics["skewness"],
                kurtosis=distribution_metrics["kurtosis"],
                
                # Assessment global
                overall_risk_level=risk_assessment["level"],
                risk_score=risk_assessment["score"],
                
                # Metadata
                calculation_date=datetime.now(),
                data_points=len(returns_data),
                confidence_level=min(1.0, len(returns_data) / 30.0)  # Plus de donn√©es = plus de confiance
            )
            
            logger.info(f"M√©triques calcul√©es: VaR 95%={metrics.var_95_1d:.1%}, "
                       f"Volatilit√©={metrics.volatility_annualized:.1%}, "
                       f"Risque={risk_assessment['level'].value}")
            
            return metrics
            
        except Exception as e:
            logger.error(f"Erreur calcul m√©triques risque: {e}")
            return RiskMetrics(confidence_level=0.0)
    
    async def _generate_historical_returns(
        self, 
        holdings: List[Dict[str, Any]], 
        days: int
    ) -> List[Dict[str, float]]:
        """G√©n√®re l'historique des returns depuis les vraies donn√©es de prix en cache"""
        
        # Extraire les symboles des holdings
        symbols = [h.get("symbol", "") for h in holdings if h.get("symbol")]
        if not symbols:
            logger.warning("Aucun symbole trouv√© dans les holdings")
            return []
        
        # Importer le module de cache d'historique
        try:
            from services.price_history import get_cached_history, calculate_returns
        except ImportError as e:
            logger.error(f"Impossible d'importer price_history: {e}")
            return await self._generate_historical_returns_fallback(symbols, days)
        
        logger.info(f"üìà Calcul rendements r√©els depuis cache pour {len(symbols)} symboles ({days}j)")
        
        # Collecter l'historique pour tous les symboles
        symbol_histories = {}
        available_symbols = []
        
        for symbol in symbols:
            history = get_cached_history(symbol, days + 1)  # +1 pour calculer les rendements
            if history and len(history) >= 2:  # Minimum 2 points pour calculer 1 rendement
                symbol_histories[symbol] = history
                available_symbols.append(symbol)
                logger.debug(f"‚úÖ {symbol}: {len(history)} points de prix")
            else:
                logger.warning(f"‚ö†Ô∏è {symbol}: historique insuffisant ou absent")
        
        if not available_symbols:
            logger.warning("‚ùå Aucun historique de prix disponible - fallback simulation")
            return await self._generate_historical_returns_fallback(symbols, days)
        
        # Calculer les rendements pour chaque symbole
        symbol_returns = {}
        for symbol in available_symbols:
            returns = calculate_returns(symbol_histories[symbol], log_returns=True)
            if returns:
                symbol_returns[symbol] = returns[-days:]  # Prendre les N derniers jours
        
        # Filtrer les s√©ries trop courtes (<10 rendements) pour ne pas r√©duire la fen√®tre globale
        MIN_RETURNS = 10
        symbol_returns = {s: r for s, r in symbol_returns.items() if len(r) >= MIN_RETURNS}
        if not symbol_returns:
            logger.warning("‚ùå Rendements insuffisants apr√®s filtrage - fallback simulation")
            return await self._generate_historical_returns_fallback(symbols, days)
        
        # Construire une fen√™tre cible fixe sans la r√©tr√©cir √† l'asset le plus court
        max_length = max(len(r) for r in symbol_returns.values())
        target_length = min(days, max_length)
        if target_length < MIN_RETURNS:
            logger.warning("‚ùå Fen√™tre disponible < seuil apr√®s filtrage - fallback simulation")
            return await self._generate_historical_returns_fallback(symbols, days)
        
        # Aligner/padder toutes les s√©ries sur la longueur cible (pad au d√©but avec 0.0 si n√©cessaire)
        for symbol in list(symbol_returns.keys()):
            seq = symbol_returns[symbol][-target_length:]
            if len(seq) < target_length:
                pad = [0.0] * (target_length - len(seq))
                seq = pad + seq
            symbol_returns[symbol] = seq
        
        # Construire la s√©rie temporelle de rendements
        returns_series = []
        for i in range(target_length):
            day_returns = {}
            for symbol in symbol_returns.keys():
                day_returns[symbol] = symbol_returns[symbol][i]
            
            # Pour les symboles manquants (non couverts), utiliser 0.0
            for symbol in symbols:
                if symbol not in day_returns:
                    day_returns[symbol] = 0.0
                    
            returns_series.append(day_returns)
        
        coverage = len(available_symbols) / len(symbols) * 100
        logger.info(f"‚úÖ {len(returns_series)} jours de rendements r√©els g√©n√©r√©s ({coverage:.1f}% couverture)")
        
        # Log de quelques statistiques pour validation
        if returns_series:
            sample_returns = [day.get('BTC', 0.0) for day in returns_series[-30:]]  # 30 derniers jours BTC
            if sample_returns and any(r != 0 for r in sample_returns):
                vol_annualized = np.std(sample_returns) * np.sqrt(252)
                logger.debug(f"üîç Validation BTC: volatilit√© 30j annualis√©e = {vol_annualized:.1%}")
        
        return returns_series
    
    async def _generate_historical_returns_fallback(
        self, 
        symbols: List[str], 
        days: int
    ) -> List[Dict[str, float]]:
        """Fallback avec simulation uniquement si donn√©es r√©elles indisponibles"""
        
        logger.warning(f"üü° FALLBACK: G√©n√©ration de rendements simul√©s pour {len(symbols)} symboles")
        
        returns_series = []
        
        # G√©n√©rer des returns al√©atoires mais r√©alistes pour chaque jour
        np.random.seed(42)  # Reproductibilit√©
        
        for day in range(days):
            day_returns = {}
            
            for symbol in symbols:
                # G√©n√©rer des returns r√©alistes bas√©s sur le type d'asset
                if symbol == "BTC":
                    return_val = np.random.normal(0.0005, 0.04)  # BTC: moyenne 0.05%, volatilit√© 4%
                elif symbol == "ETH":
                    return_val = np.random.normal(0.0008, 0.05)  # ETH: moyenne 0.08%, volatilit√© 5%
                elif symbol in ["USDT", "USDC", "DAI"]:
                    return_val = np.random.normal(0.0001, 0.002)  # Stablecoins: tr√®s faible volatilit√©
                else:
                    return_val = np.random.normal(0.0010, 0.08)  # Altcoins: plus volatile
                
                day_returns[symbol] = float(return_val)
            
            returns_series.append(day_returns)
        
        logger.info(f"‚ö†Ô∏è G√©n√©r√© {len(returns_series)} jours de donn√©es simul√©es (FALLBACK)")
        return returns_series
    
    def _calculate_portfolio_returns(
        self, 
        holdings: List[Dict[str, Any]], 
        returns_data: List[Dict[str, float]]
    ) -> List[float]:
        """Calcule les returns du portfolio bas√© sur les poids et returns des assets"""
        
        total_value = sum(h.get("value_usd", 0) for h in holdings)
        if total_value <= 0:
            return []
        
        # Calcul des poids
        weights = {}
        for holding in holdings:
            symbol = holding.get("symbol", "")
            weight = holding.get("value_usd", 0) / total_value
            weights[symbol] = weight
        
        # Calcul des returns pond√©r√©s du portfolio
        portfolio_returns = []
        
        for day_returns in returns_data:
            portfolio_return = 0.0
            
            for symbol, weight in weights.items():
                asset_return = day_returns.get(symbol, 0.0)
                portfolio_return += weight * asset_return
            
            portfolio_returns.append(portfolio_return)
        
        return portfolio_returns
    
    def _calculate_var_cvar(self, returns: List[float]) -> Dict[str, float]:
        """Calcule Value at Risk et Conditional VaR"""
        
        if len(returns) < 10:
            return {"var_95": 0.0, "var_99": 0.0, "cvar_95": 0.0, "cvar_99": 0.0}
        
        returns_array = np.array(returns)
        
        # VaR historique (percentiles)
        var_95 = -np.percentile(returns_array, 5)   # 5% pire cas
        var_99 = -np.percentile(returns_array, 1)   # 1% pire cas
        
        # CVaR (Expected Shortfall) - moyenne des returns au-del√† du VaR
        cvar_95 = -np.mean(returns_array[returns_array <= -var_95]) if var_95 > 0 else 0.0
        cvar_99 = -np.mean(returns_array[returns_array <= -var_99]) if var_99 > 0 else 0.0
        
        return {
            "var_95": var_95,
            "var_99": var_99, 
            "cvar_95": cvar_95,
            "cvar_99": cvar_99
        }
    
    def _calculate_risk_adjusted_metrics(self, returns: List[float]) -> Dict[str, float]:
        """Calcule Sharpe, Sortino, Calmar ratios"""
        
        if len(returns) < 10:
            return {"volatility": 0.0, "sharpe": 0.0, "sortino": 0.0, "calmar": 0.0}
        
        returns_array = np.array(returns)
        
        # M√©triques de base
        mean_return = np.mean(returns_array)
        volatility = np.std(returns_array) * np.sqrt(252)  # Annualis√©
        
        # Sharpe ratio
        risk_free_daily = self.risk_free_rate / 252
        excess_return = mean_return - risk_free_daily
        sharpe = (excess_return * 252) / volatility if volatility > 0 else 0.0
        
        # Sortino ratio (downside deviation)
        downside_returns = returns_array[returns_array < 0]
        downside_deviation = np.std(downside_returns) * np.sqrt(252) if len(downside_returns) > 0 else 0.0
        sortino = (excess_return * 252) / downside_deviation if downside_deviation > 0 else 0.0
        
        # Calmar ratio n√©cessite max drawdown (calcul√© s√©par√©ment)
        cumulative_returns = np.cumprod(1 + returns_array)
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdowns = (cumulative_returns - running_max) / running_max
        max_drawdown = abs(np.min(drawdowns))
        
        annual_return = mean_return * 252
        calmar = annual_return / max_drawdown if max_drawdown > 0 else 0.0
        
        return {
            "volatility": volatility,
            "sharpe": sharpe,
            "sortino": sortino, 
            "calmar": calmar
        }
    
    def _calculate_drawdown_metrics(self, returns: List[float]) -> Dict[str, Any]:
        """Calcule les m√©triques de drawdown"""
        
        if len(returns) < 10:
            return {
                "max_drawdown": 0.0,
                "max_duration": 0,
                "current_drawdown": 0.0,
                "ulcer_index": 0.0
            }
        
        returns_array = np.array(returns)
        
        # Calcul des drawdowns
        cumulative_returns = np.cumprod(1 + returns_array)
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdowns = (cumulative_returns - running_max) / running_max
        
        # Max drawdown
        max_drawdown = abs(np.min(drawdowns))
        
        # Dur√©e du max drawdown
        max_dd_duration = 0
        current_duration = 0
        
        for dd in drawdowns:
            if dd < -0.001:  # Seuil de drawdown significatif
                current_duration += 1
            else:
                max_dd_duration = max(max_dd_duration, current_duration)
                current_duration = 0
        max_dd_duration = max(max_dd_duration, current_duration)
        
        # Current drawdown
        current_drawdown = abs(drawdowns[-1]) if len(drawdowns) > 0 else 0.0
        
        # Ulcer Index (pain index)
        squared_drawdowns = drawdowns ** 2
        ulcer_index = np.sqrt(np.mean(squared_drawdowns))
        
        return {
            "max_drawdown": max_drawdown,
            "max_duration": max_dd_duration,
            "current_drawdown": current_drawdown,
            "ulcer_index": ulcer_index
        }
    
    def _calculate_distribution_metrics(self, returns: List[float]) -> Dict[str, float]:
        """Calcule skewness et kurtosis"""
        
        if len(returns) < 10:
            return {"skewness": 0.0, "kurtosis": 0.0}
        
        returns_array = np.array(returns)
        
        skewness = stats.skew(returns_array)
        kurtosis = stats.kurtosis(returns_array)  # Excess kurtosis
        
        return {
            "skewness": skewness,
            "kurtosis": kurtosis
        }
    
    def _assess_overall_risk_level(
        self, 
        var_metrics: Dict[str, float],
        perf_metrics: Dict[str, float], 
        drawdown_metrics: Dict[str, Any]
    ) -> Dict[str, Any]:
        """√âvalue le niveau de risque global du portfolio"""
        
        # Score bas√© sur diff√©rents crit√®res
        score = 50.0  # Score de base
        
        # VaR impact
        var_95 = var_metrics.get("var_95", 0.0)
        if var_95 > 0.25:
            score += 30
        elif var_95 > 0.15:
            score += 20
        elif var_95 > 0.10:
            score += 10
        elif var_95 < 0.05:
            score -= 10
        
        # Volatilit√© impact
        vol = perf_metrics.get("volatility", 0.0)
        if vol > 1.0:
            score += 25
        elif vol > 0.6:
            score += 15
        elif vol > 0.4:
            score += 5
        elif vol < 0.2:
            score -= 15
        
        # Max drawdown impact
        max_dd = drawdown_metrics.get("max_drawdown", 0.0)
        if max_dd > 0.50:
            score += 20
        elif max_dd > 0.30:
            score += 10
        elif max_dd < 0.10:
            score -= 10
        
        # Sharpe ratio impact (inverse)
        sharpe = perf_metrics.get("sharpe", 0.0)
        if sharpe < 0:
            score += 15
        elif sharpe > 1.5:
            score -= 15
        elif sharpe > 1.0:
            score -= 10
        
        # Normaliser le score
        score = max(0, min(100, score))
        
        # D√©terminer le niveau de risque
        if score >= 80:
            level = RiskLevel.CRITICAL
        elif score >= 65:
            level = RiskLevel.VERY_HIGH
        elif score >= 50:
            level = RiskLevel.HIGH
        elif score >= 35:
            level = RiskLevel.MEDIUM
        elif score >= 20:
            level = RiskLevel.LOW
        else:
            level = RiskLevel.VERY_LOW
        
        return {
            "level": level,
            "score": score
        }
    
    async def calculate_correlation_matrix(
        self, 
        holdings: List[Dict[str, Any]],
        lookback_days: int = 30
    ) -> CorrelationMatrix:
        """Calcule la matrice de corr√©lation entre assets avec analyse en composantes principales"""
        
        try:
            logger.info(f"Calcul matrice corr√©lation pour {len(holdings)} assets")
            
            # G√©n√©rer historique de returns
            returns_data = await self._generate_historical_returns(holdings, lookback_days)
            
            if len(returns_data) < 10:
                return CorrelationMatrix()
            
            # Construire matrice des returns et filtrer les symboles sans variance
            all_symbols = [h.get("symbol", "") for h in holdings]
            returns_matrix = []
            for day_returns in returns_data:
                returns_matrix.append([day_returns.get(symbol, 0.0) for symbol in all_symbols])
            
            df = pd.DataFrame(returns_matrix, columns=all_symbols)
            # Garder uniquement les colonnes avec une variance non nulle (√©vite NaN dans corr())
            variances = df.var(axis=0, ddof=1)
            symbols = [sym for sym in all_symbols if sym and float(variances.get(sym, 0.0)) > 0.0]
            if not symbols:
                logger.warning("Aucun symbole avec variance non nulle pour la corr√©lation ‚Üí retour vide")
                return CorrelationMatrix()
            if len(symbols) == 1:
                sym = symbols[0]
                return CorrelationMatrix(
                    correlations={sym: {sym: 1.0}},
                    eigen_values=[1.0],
                    eigen_vectors=[[1.0]],
                    principal_components={"PC1": 1.0},
                    diversification_ratio=1.0,
                    effective_assets=1.0,
                    last_updated=datetime.now()
                )
            returns_df = df[symbols]
            
            # Calcul de la matrice de corr√©lation
            corr_matrix = returns_df.corr().fillna(0.0)
            # Assurer diagonale √† 1 et clip [-1, 1] pour stabilit√©
            for i in range(len(symbols)):
                corr_matrix.iat[i, i] = 1.0
            corr_matrix = corr_matrix.clip(lower=-1.0, upper=1.0)
            
            # Conversion en dictionnaire
            correlations = {}
            for symbol1 in symbols:
                correlations[symbol1] = {}
                for symbol2 in symbols:
                    correlations[symbol1][symbol2] = corr_matrix.loc[symbol1, symbol2]
            
            # Analyse en composantes principales
            # S√©curiser la d√©composition en pr√©sence de petits artefacts num√©riques
            safe_matrix = np.nan_to_num(corr_matrix.values, nan=0.0, posinf=1.0, neginf=-1.0)
            try:
                eigen_values, eigen_vectors = np.linalg.eigh(safe_matrix)
            except Exception as _e:
                n = len(symbols)
                eigen_values = np.ones(n)
                eigen_vectors = np.eye(n)
            eigen_values = eigen_values.tolist()
            eigen_vectors = eigen_vectors.tolist()
            
            # Principal components (variance expliqu√©e)
            total_variance = sum(eigen_values)
            principal_components = {}
            for i, eigenval in enumerate(eigen_values):
                principal_components[f"PC{i+1}"] = eigenval / total_variance if total_variance > 0 else 0
            
            # Ratio de diversification
            # Recalculer les poids uniquement sur les symboles actifs
            value_map = {h.get("symbol", ""): float(h.get("value_usd", 0)) for h in holdings}
            weights_arr = np.array([value_map.get(sym, 0.0) for sym in symbols], dtype=float)
            if weights_arr.sum() <= 0:
                weights_arr = np.ones(len(symbols), dtype=float)
            portfolio_weights = weights_arr / weights_arr.sum()
            
            portfolio_variance = float(np.dot(portfolio_weights, np.dot(safe_matrix, portfolio_weights)))
            weighted_avg_variance = float(np.dot(portfolio_weights**2, np.ones(len(symbols))))
            diversification_ratio = np.sqrt(weighted_avg_variance / portfolio_variance) if portfolio_variance > 0 else 1.0
            
            # Nombre effectif d'assets (inverse participation ratio)
            effective_assets = 1.0 / np.sum(portfolio_weights**4) if len(portfolio_weights) > 0 else 1.0
            
            correlation_matrix = CorrelationMatrix(
                correlations=correlations,
                eigen_values=eigen_values,
                eigen_vectors=eigen_vectors,
                principal_components=principal_components,
                diversification_ratio=diversification_ratio,
                effective_assets=effective_assets,
                last_updated=datetime.now()
            )
            
            logger.info(f"Corr√©lation calcul√©e: ratio diversification={diversification_ratio:.2f}, "
                       f"assets effectifs={effective_assets:.1f}")
            
            return correlation_matrix
            
        except Exception as e:
            logger.error(f"Erreur calcul matrice corr√©lation: {e}")
            return CorrelationMatrix()
    
    async def run_stress_test(
        self, 
        holdings: List[Dict[str, Any]], 
        scenario: StressScenario,
        custom_shocks: Optional[Dict[str, float]] = None
    ) -> StressTestResult:
        """Ex√©cute un stress test sur le portfolio"""
        
        try:
            scenario_config = self.stress_scenarios.get(scenario, {})
            if not scenario_config and not custom_shocks:
                raise ValueError(f"Sc√©nario {scenario} non trouv√© et pas de shocks personnalis√©s")
            
            logger.info(f"Stress test: {scenario.value}")
            
            # Utiliser shocks personnalis√©s ou pr√©d√©finis
            if custom_shocks:
                asset_shocks = custom_shocks
                scenario_name = "Custom Scenario"
                scenario_desc = "Sc√©nario personnalis√© d√©fini par l'utilisateur"
            else:
                asset_shocks = scenario_config["asset_shocks"]
                scenario_name = scenario_config["name"]
                scenario_desc = scenario_config["description"]
            
            # Calcul des impacts par asset
            taxonomy = Taxonomy.load()
            total_portfolio_value = sum(h.get("value_usd", 0) for h in holdings)
            
            asset_impacts = []
            total_loss = 0.0
            
            for holding in holdings:
                symbol = holding.get("symbol", "")
                alias = holding.get("alias", symbol)
                group = taxonomy.group_for_alias(alias)
                value = holding.get("value_usd", 0)
                
                # Trouver le shock applicable
                shock = asset_shocks.get(group, asset_shocks.get("Others", -0.30))
                
                # Calculer l'impact
                loss_amount = value * abs(shock)
                loss_pct = shock
                
                asset_impacts.append({
                    "symbol": symbol,
                    "group": group,
                    "value_before": value,
                    "shock_pct": loss_pct,
                    "loss_usd": loss_amount,
                    "value_after": value * (1 + shock)
                })
                
                total_loss += loss_amount
            
            # Trier par impact (pires performances)
            asset_impacts.sort(key=lambda x: x["shock_pct"])
            
            worst_performers = asset_impacts[:3]
            best_performers = asset_impacts[-3:]
            
            # Calcul du loss total du portfolio
            portfolio_loss_pct = total_loss / total_portfolio_value if total_portfolio_value > 0 else 0.0
            
            # V√©rifier si VaR est d√©pass√© (n√©cessite calcul pr√©alable)
            current_metrics = await self.calculate_portfolio_risk_metrics(holdings)
            var_breach = portfolio_loss_pct > current_metrics.var_99_1d
            
            # Estimation temps de r√©cup√©ration (bas√© sur sc√©narios historiques)
            if scenario_config:
                base_recovery = scenario_config.get("duration_days", 90)
                # Ajustement selon la s√©v√©rit√©
                severity_multiplier = min(3.0, portfolio_loss_pct / 0.20)  # 20% = 1x
                recovery_time = int(base_recovery * severity_multiplier)
            else:
                recovery_time = int(90 * (portfolio_loss_pct / 0.20))  # Estimation basique
            
            # Contribution au risque par asset
            risk_contributions = {}
            for impact in asset_impacts:
                contribution = abs(impact["loss_usd"]) / total_loss if total_loss > 0 else 0.0
                risk_contributions[impact["symbol"]] = contribution
            
            result = StressTestResult(
                scenario_name=scenario_name,
                scenario_description=scenario_desc,
                portfolio_loss_pct=portfolio_loss_pct,
                portfolio_loss_usd=total_loss,
                worst_performing_assets=worst_performers,
                best_performing_assets=best_performers,
                var_breach=var_breach,
                recovery_time_estimate_days=recovery_time,
                risk_contribution=risk_contributions
            )
            
            logger.info(f"Stress test termin√©: perte {portfolio_loss_pct:.1%} (${total_loss:,.0f})")
            return result
            
        except Exception as e:
            logger.error(f"Erreur stress test: {e}")
            return StressTestResult(
                scenario_name="Error",
                scenario_description=f"Erreur: {str(e)}",
                portfolio_loss_pct=0.0,
                portfolio_loss_usd=0.0,
                worst_performing_assets=[],
                best_performing_assets=[],
                var_breach=False,
                recovery_time_estimate_days=0,
                risk_contribution={}
            )

    async def calculate_performance_attribution(
        self,
        holdings: List[Dict[str, Any]],
        analysis_days: int = 30,
        benchmark_portfolio: Optional[List[Dict[str, Any]]] = None
    ) -> PerformanceAttribution:
        """
        Calcule l'attribution de performance d√©taill√©e du portfolio
        
        D√©compose la performance en contributions individuelles par asset et par groupe,
        incluant les effets de s√©lection, allocation et interaction
        
        Args:
            holdings: Holdings actuels du portfolio
            analysis_days: P√©riode d'analyse en jours
            benchmark_portfolio: Portfolio de r√©f√©rence pour comparaison (optionnel)
            
        Returns:
            PerformanceAttribution avec analyse d√©taill√©e
        """
        try:
            logger.info(f"Calcul attribution performance sur {analysis_days} jours")
            
            # 1. Validation des donn√©es
            portfolio_value = sum(h.get("value_usd", 0) for h in holdings)
            if portfolio_value <= 0:
                return PerformanceAttribution(total_return=0.0, total_return_usd=0.0)
            
            # 2. G√©n√©ration historique des returns
            returns_data = await self._generate_historical_returns(holdings, analysis_days)
            if len(returns_data) < 2:
                return PerformanceAttribution(total_return=0.0, total_return_usd=0.0)
            
            # 3. Calcul des returns et contributions individuelles par asset
            asset_contributions = self._calculate_asset_contributions(holdings, returns_data, portfolio_value)
            
            # 4. Agr√©gation par groupe d'assets
            group_contributions = self._calculate_group_contributions(holdings, asset_contributions)
            
            # 5. Calcul des effets d'attribution (s√©lection, allocation, interaction)
            attribution_effects = self._calculate_attribution_effects(
                asset_contributions, 
                group_contributions,
                benchmark_portfolio,
                returns_data
            )
            
            # 6. Calcul du return total du portfolio
            total_return_pct = sum(contrib["contribution_pct"] for contrib in asset_contributions)
            total_return_usd = sum(contrib["contribution_usd"] for contrib in asset_contributions)
            
            # 7. Construction du r√©sultat
            period_end = datetime.now()
            period_start = period_end - timedelta(days=analysis_days)
            
            attribution = PerformanceAttribution(
                total_return=total_return_pct,
                total_return_usd=total_return_usd,
                asset_contributions=asset_contributions,
                group_contributions=group_contributions,
                selection_effect=attribution_effects["selection"],
                allocation_effect=attribution_effects["allocation"], 
                interaction_effect=attribution_effects["interaction"],
                period_start=period_start,
                period_end=period_end,
                analysis_period_days=analysis_days,
                calculation_date=datetime.now(),
                benchmark_used="Equal Weight" if benchmark_portfolio is None else "Custom"
            )
            
            logger.info(f"Attribution calcul√©e: return total {total_return_pct:.2%} "
                       f"(${total_return_usd:,.0f}) sur {analysis_days} jours")
            
            return attribution
            
        except Exception as e:
            logger.error(f"Erreur calcul attribution performance: {e}")
            return PerformanceAttribution(total_return=0.0, total_return_usd=0.0)
    
    def _calculate_asset_contributions(
        self,
        holdings: List[Dict[str, Any]],
        returns_data: List[Dict[str, float]],
        portfolio_value: float
    ) -> List[Dict[str, Any]]:
        """Calcule les contributions individuelles de chaque asset"""
        
        asset_contributions = []
        
        for holding in holdings:
            symbol = holding.get("symbol", "")
            value = holding.get("value_usd", 0)
            weight = value / portfolio_value if portfolio_value > 0 else 0
            
            # Calculer le return de l'asset sur la p√©riode
            asset_returns = [day.get(symbol, 0.0) for day in returns_data]
            if len(asset_returns) == 0:
                continue
                
            # Return cumul√© de l'asset
            cumulative_return = np.prod([1 + r for r in asset_returns]) - 1
            
            # Contribution √† la performance du portfolio = poids √ó return
            contribution_pct = weight * cumulative_return
            contribution_usd = value * cumulative_return
            
            # Volatilit√© et Sharpe de l'asset
            asset_volatility = np.std(asset_returns) * np.sqrt(252) if len(asset_returns) > 1 else 0.0
            asset_sharpe = (np.mean(asset_returns) * 252) / asset_volatility if asset_volatility > 0 else 0.0
            
            # Classification taxonomique
            from services.taxonomy import Taxonomy
            taxonomy = Taxonomy.load()
            alias = holding.get("alias", symbol)
            group = taxonomy.group_for_alias(alias)
            
            asset_contributions.append({
                "symbol": symbol,
                "alias": alias,
                "group": group,
                "weight": weight,
                "value_usd": value,
                "asset_return": cumulative_return,
                "contribution_pct": contribution_pct,
                "contribution_usd": contribution_usd,
                "volatility": asset_volatility,
                "sharpe_ratio": asset_sharpe,
                "daily_returns": asset_returns
            })
        
        # Trier par contribution d√©croissante
        asset_contributions.sort(key=lambda x: x["contribution_pct"], reverse=True)
        
        return asset_contributions
    
    def _calculate_group_contributions(
        self,
        holdings: List[Dict[str, Any]],
        asset_contributions: List[Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """Agr√®ge les contributions par groupe d'assets"""
        
        group_stats = {}
        
        # Regrouper par groupe taxonomique
        for contrib in asset_contributions:
            group = contrib["group"]
            
            if group not in group_stats:
                group_stats[group] = {
                    "group_name": group,
                    "num_assets": 0,
                    "total_weight": 0.0,
                    "total_value_usd": 0.0,
                    "group_return": 0.0,
                    "contribution_pct": 0.0,
                    "contribution_usd": 0.0,
                    "average_volatility": 0.0,
                    "group_sharpe": 0.0,
                    "assets": []
                }
            
            # Accumulation des statistiques
            stats = group_stats[group]
            stats["num_assets"] += 1
            stats["total_weight"] += contrib["weight"]
            stats["total_value_usd"] += contrib["value_usd"]
            stats["contribution_pct"] += contrib["contribution_pct"]
            stats["contribution_usd"] += contrib["contribution_usd"]
            stats["assets"].append(contrib["symbol"])
        
        # Calcul des m√©triques moyennes par groupe
        for group, stats in group_stats.items():
            if stats["num_assets"] > 0:
                # Return moyen pond√©r√© du groupe
                if stats["total_weight"] > 0:
                    weighted_return = stats["contribution_pct"] / stats["total_weight"]
                    stats["group_return"] = weighted_return
                
                # Volatilit√© moyenne du groupe
                group_assets = [c for c in asset_contributions if c["group"] == group]
                if group_assets:
                    weighted_volatilities = sum(c["weight"] * c["volatility"] for c in group_assets)
                    stats["average_volatility"] = weighted_volatilities / stats["total_weight"] if stats["total_weight"] > 0 else 0.0
                    
                    # Sharpe moyen du groupe
                    weighted_sharpes = sum(c["weight"] * c["sharpe_ratio"] for c in group_assets if not np.isnan(c["sharpe_ratio"]))
                    stats["group_sharpe"] = weighted_sharpes / stats["total_weight"] if stats["total_weight"] > 0 else 0.0
        
        return group_stats
    
    def _calculate_attribution_effects(
        self,
        asset_contributions: List[Dict[str, Any]],
        group_contributions: Dict[str, Dict[str, Any]], 
        benchmark_portfolio: Optional[List[Dict[str, Any]]],
        returns_data: List[Dict[str, float]]
    ) -> Dict[str, float]:
        """
        Calcule les effets d'attribution Brinson-style:
        - Selection Effect: Impact du choix d'assets sp√©cifiques dans chaque groupe
        - Allocation Effect: Impact de la sur/sous-pond√©ration de groupes
        - Interaction Effect: Effet combin√© allocation √ó s√©lection
        """
        
        # Benchmark par d√©faut: equal weight sur tous les groupes
        if benchmark_portfolio is None:
            # Cr√©er un benchmark equal-weight
            unique_groups = set(contrib["group"] for contrib in asset_contributions)
            benchmark_weight_per_group = 1.0 / len(unique_groups) if unique_groups else 0.0
            benchmark_weights = {group: benchmark_weight_per_group for group in unique_groups}
        else:
            # Utiliser le benchmark fourni (non impl√©ment√© pour la d√©mo)
            benchmark_weights = {}
        
        # Calcul des returns de benchmark par groupe (moyenne des assets du groupe)
        benchmark_returns = {}
        for group in group_contributions.keys():
            group_assets = [c for c in asset_contributions if c["group"] == group]
            if group_assets:
                # Return moyen non-pond√©r√© des assets du groupe (benchmark)
                avg_return = np.mean([c["asset_return"] for c in group_assets])
                benchmark_returns[group] = avg_return
            else:
                benchmark_returns[group] = 0.0
        
        # Calcul des effets
        allocation_effect = 0.0
        selection_effect = 0.0
        interaction_effect = 0.0
        
        for group, stats in group_contributions.items():
            portfolio_weight = stats["total_weight"]
            benchmark_weight = benchmark_weights.get(group, portfolio_weight)
            portfolio_return = stats["group_return"]
            benchmark_return = benchmark_returns.get(group, 0.0)
            
            # Allocation Effect = (Portfolio Weight - Benchmark Weight) √ó Benchmark Return
            allocation_effect += (portfolio_weight - benchmark_weight) * benchmark_return
            
            # Selection Effect = Benchmark Weight √ó (Portfolio Return - Benchmark Return)
            selection_effect += benchmark_weight * (portfolio_return - benchmark_return)
            
            # Interaction Effect = (Portfolio Weight - Benchmark Weight) √ó (Portfolio Return - Benchmark Return)
            interaction_effect += (portfolio_weight - benchmark_weight) * (portfolio_return - benchmark_return)
        
        return {
            "allocation": allocation_effect,
            "selection": selection_effect,
            "interaction": interaction_effect
        }
    
    async def run_strategy_backtest(
        self,
        strategy_name: str,
        target_allocations: Dict[str, float],
        backtest_days: int = 180,
        rebalance_frequency_days: int = 30,
        transaction_cost_pct: float = 0.001  # 0.1% de co√ªt par transaction
    ) -> BacktestResult:
        """
        Ex√©cute un backtest d'une strat√©gie d'allocation sur historique simul√©
        
        Args:
            strategy_name: Nom de la strat√©gie
            target_allocations: Allocations cibles par groupe d'assets (ex: {"BTC": 0.4, "ETH": 0.3, "DeFi": 0.3})
            backtest_days: Nombre de jours de backtest
            rebalance_frequency_days: Fr√©quence de rebalancing en jours
            transaction_cost_pct: Co√ªt de transaction en % du volume trad√©
            
        Returns:
            BacktestResult avec performance et m√©triques d√©taill√©es
        """
        try:
            logger.info(f"D√©but backtest strat√©gie '{strategy_name}' sur {backtest_days} jours")
            
            # 1. Validation des allocations cibles
            total_allocation = sum(target_allocations.values())
            if abs(total_allocation - 1.0) > 0.01:
                raise ValueError(f"Allocations cibles doivent sommer √† 100% (actuellement {total_allocation:.1%})")
            
            # 2. G√©n√©ration de l'univers d'assets et historique de prix
            universe_holdings = self._generate_asset_universe(target_allocations)
            price_history = await self._generate_historical_returns(universe_holdings, backtest_days + 30)
            
            if len(price_history) < backtest_days:
                raise ValueError("Pas assez de donn√©es historiques pour le backtest")
            
            # 3. Simulation du backtest jour par jour
            backtest_result = await self._simulate_backtest(
                strategy_name=strategy_name,
                target_allocations=target_allocations, 
                universe_holdings=universe_holdings,
                price_history=price_history[-backtest_days:],  # Prendre les derniers jours
                rebalance_frequency=rebalance_frequency_days,
                transaction_cost=transaction_cost_pct
            )
            
            logger.info(f"Backtest termin√©: return {backtest_result.total_return:.2%}, "
                       f"Sharpe {backtest_result.sharpe_ratio:.2f}, "
                       f"Max DD {backtest_result.max_drawdown:.2%}")
            
            return backtest_result
            
        except Exception as e:
            logger.error(f"Erreur backtest: {e}")
            return BacktestResult(
                strategy_name=strategy_name,
                strategy_description=f"Erreur: {str(e)}",
                total_return=0.0,
                annualized_return=0.0,
                volatility=0.0,
                sharpe_ratio=0.0,
                max_drawdown=0.0,
                benchmark_return=0.0,
                active_return=0.0,
                information_ratio=0.0,
                tracking_error=0.0,
                var_95=0.0,
                downside_deviation=0.0,
                sortino_ratio=0.0,
                calmar_ratio=0.0,
                num_rebalances=0,
                avg_turnover=0.0,
                total_costs=0.0,
                backtest_days=0
            )
    
    def _generate_asset_universe(self, target_allocations: Dict[str, float]) -> List[Dict[str, Any]]:
        """G√©n√®re un univers d'assets repr√©sentatifs pour chaque groupe d'allocation"""
        
        universe = []
        
        # Mapping des groupes vers des assets repr√©sentatifs
        group_representatives = {
            "BTC": [{"symbol": "BTC", "alias": "bitcoin", "value_usd": 10000}],
            "ETH": [{"symbol": "ETH", "alias": "ethereum", "value_usd": 10000}],
            "Stablecoins": [
                {"symbol": "USDT", "alias": "tether", "value_usd": 5000},
                {"symbol": "USDC", "alias": "usd-coin", "value_usd": 5000}
            ],
            "L1/L0 majors": [
                {"symbol": "SOL", "alias": "solana", "value_usd": 3000},
                {"symbol": "ADA", "alias": "cardano", "value_usd": 3000},
                {"symbol": "DOT", "alias": "polkadot", "value_usd": 2000}
            ],
            "L2/Scaling": [
                {"symbol": "MATIC", "alias": "polygon", "value_usd": 2000},
                {"symbol": "AVAX", "alias": "avalanche", "value_usd": 2000}
            ],
            "DeFi": [
                {"symbol": "UNI", "alias": "uniswap", "value_usd": 2000},
                {"symbol": "AAVE", "alias": "aave", "value_usd": 2000},
                {"symbol": "LINK", "alias": "chainlink", "value_usd": 2000}
            ],
            "AI/Data": [
                {"symbol": "FET", "alias": "fetch-ai", "value_usd": 1000},
                {"symbol": "OCEAN", "alias": "ocean-protocol", "value_usd": 1000}
            ],
            "Gaming/NFT": [
                {"symbol": "AXS", "alias": "axie-infinity", "value_usd": 1000},
                {"symbol": "SAND", "alias": "the-sandbox", "value_usd": 1000}
            ],
            "Memecoins": [
                {"symbol": "DOGE", "alias": "dogecoin", "value_usd": 500},
                {"symbol": "SHIB", "alias": "shiba-inu", "value_usd": 500}
            ]
        }
        
        # Cr√©er l'univers bas√© sur les allocations cibles
        for group, allocation in target_allocations.items():
            if group in group_representatives:
                universe.extend(group_representatives[group])
            else:
                # Groupe inconnu, cr√©er un asset g√©n√©rique
                universe.append({
                    "symbol": f"{group}_REP", 
                    "alias": group.lower().replace(" ", "-"),
                    "value_usd": allocation * 10000  # Base 10k portfolio
                })
        
        return universe
    
    async def _simulate_backtest(
        self,
        strategy_name: str,
        target_allocations: Dict[str, float],
        universe_holdings: List[Dict[str, Any]],
        price_history: List[Dict[str, float]], 
        rebalance_frequency: int,
        transaction_cost: float
    ) -> BacktestResult:
        """Simule le backtest jour par jour avec rebalancing p√©riodique"""
        
        # Initialisation
        initial_portfolio_value = 100000.0  # Portfolio de 100k$ initial
        portfolio_values = [initial_portfolio_value]
        benchmark_values = [initial_portfolio_value]
        dates = []
        rebalancing_dates = []
        
        # √âtat du portfolio et benchmark
        current_holdings = {}  # symbol -> quantity
        current_cash = 0.0
        total_costs = 0.0
        num_rebalances = 0
        
        # Benchmark: equal weight sur tous les assets
        benchmark_weights = {h["symbol"]: 1.0 / len(universe_holdings) for h in universe_holdings}
        benchmark_holdings = {}
        
        # Classification taxonomique pour mappage
        from services.taxonomy import Taxonomy
        taxonomy = Taxonomy.load()
        
        # Initialisation des holdings (jour 0)
        for holding in universe_holdings:
            symbol = holding["symbol"]
            alias = holding.get("alias", symbol)
            group = taxonomy.group_for_alias(alias)
            
            # Allocation cible pour ce groupe
            target_weight = target_allocations.get(group, 0.0)
            target_value = initial_portfolio_value * target_weight / sum(
                1 for h in universe_holdings 
                if taxonomy.group_for_alias(h.get("alias", h["symbol"])) == group
            )
            
            # Prix initial (jour 0 = r√©f√©rence)
            initial_price = 100.0  # Prix de r√©f√©rence
            quantity = target_value / initial_price
            
            current_holdings[symbol] = quantity
            benchmark_holdings[symbol] = (initial_portfolio_value * benchmark_weights[symbol]) / initial_price
        
        # Simulation jour par jour
        for day_idx, day_returns in enumerate(price_history):
            current_date = datetime.now() - timedelta(days=len(price_history) - day_idx - 1)
            dates.append(current_date)
            
            # Calcul de la valeur actuelle du portfolio et benchmark
            portfolio_value = current_cash
            benchmark_value = 0.0
            
            for symbol, quantity in current_holdings.items():
                if symbol in day_returns and quantity > 0:
                    # Prix actuel = prix initial √ó prod(1 + return_daily) 
                    cumulative_return = np.prod([1 + price_history[i].get(symbol, 0.0) for i in range(day_idx + 1)])
                    current_price = 100.0 * cumulative_return
                    portfolio_value += quantity * current_price
            
            for symbol, quantity in benchmark_holdings.items():
                if symbol in day_returns and quantity > 0:
                    cumulative_return = np.prod([1 + price_history[i].get(symbol, 0.0) for i in range(day_idx + 1)])
                    current_price = 100.0 * cumulative_return
                    benchmark_value += quantity * current_price
            
            portfolio_values.append(portfolio_value)
            benchmark_values.append(benchmark_value)
            
            # Rebalancing p√©riodique
            if day_idx > 0 and day_idx % rebalance_frequency == 0:
                rebalancing_dates.append(current_date)
                
                # Calculer les nouveaux targets
                for holding in universe_holdings:
                    symbol = holding["symbol"]
                    alias = holding.get("alias", symbol)
                    group = taxonomy.group_for_alias(alias)
                    
                    target_weight = target_allocations.get(group, 0.0)
                    assets_in_group = sum(
                        1 for h in universe_holdings 
                        if taxonomy.group_for_alias(h.get("alias", h["symbol"])) == group
                    )
                    target_value = portfolio_value * target_weight / max(1, assets_in_group)
                    
                    # Prix actuel
                    cumulative_return = np.prod([1 + price_history[i].get(symbol, 0.0) for i in range(day_idx + 1)])
                    current_price = 100.0 * cumulative_return
                    
                    target_quantity = target_value / current_price if current_price > 0 else 0
                    current_quantity = current_holdings.get(symbol, 0)
                    
                    # Trade et co√ªts
                    trade_quantity = abs(target_quantity - current_quantity)
                    trade_cost = trade_quantity * current_price * transaction_cost
                    total_costs += trade_cost
                    
                    current_holdings[symbol] = target_quantity
                
                num_rebalances += 1
        
        # Calcul des m√©triques finales
        portfolio_returns = [
            (portfolio_values[i] / portfolio_values[i-1] - 1) 
            for i in range(1, len(portfolio_values))
        ]
        
        benchmark_returns = [
            (benchmark_values[i] / benchmark_values[i-1] - 1) 
            for i in range(1, len(benchmark_values))
        ]
        
        # Performance metrics
        total_return = (portfolio_values[-1] / portfolio_values[0]) - 1
        benchmark_total_return = (benchmark_values[-1] / benchmark_values[0]) - 1
        
        trading_days = len(portfolio_returns)
        annualized_return = (1 + total_return) ** (252 / trading_days) - 1 if trading_days > 0 else 0.0
        volatility = np.std(portfolio_returns) * np.sqrt(252) if len(portfolio_returns) > 1 else 0.0
        
        # Sharpe ratio
        risk_free_rate = 0.02
        excess_return = annualized_return - risk_free_rate
        sharpe_ratio = excess_return / volatility if volatility > 0 else 0.0
        
        # Max drawdown
        portfolio_series = np.array(portfolio_values)
        running_max = np.maximum.accumulate(portfolio_series)
        drawdowns = (portfolio_series - running_max) / running_max
        max_drawdown = abs(np.min(drawdowns))
        
        # Active return et tracking error
        active_returns = [p_ret - b_ret for p_ret, b_ret in zip(portfolio_returns, benchmark_returns)]
        active_return = annualized_return - ((1 + benchmark_total_return) ** (252 / trading_days) - 1)
        tracking_error = np.std(active_returns) * np.sqrt(252) if len(active_returns) > 1 else 0.0
        information_ratio = active_return / tracking_error if tracking_error > 0 else 0.0
        
        # Risk metrics
        var_95 = -np.percentile(portfolio_returns, 5) if len(portfolio_returns) >= 20 else 0.0
        downside_returns = [r for r in portfolio_returns if r < 0]
        downside_deviation = np.std(downside_returns) * np.sqrt(252) if len(downside_returns) > 1 else 0.0
        sortino_ratio = excess_return / downside_deviation if downside_deviation > 0 else 0.0
        calmar_ratio = annualized_return / max_drawdown if max_drawdown > 0 else 0.0
        
        # Turnover moyen
        avg_turnover = total_costs / (num_rebalances * np.mean(portfolio_values)) if num_rebalances > 0 else 0.0
        
        return BacktestResult(
            strategy_name=strategy_name,
            strategy_description=f"Allocation cible: {target_allocations}",
            total_return=total_return,
            annualized_return=annualized_return,
            volatility=volatility,
            sharpe_ratio=sharpe_ratio,
            max_drawdown=max_drawdown,
            benchmark_return=benchmark_total_return,
            active_return=active_return,
            information_ratio=information_ratio,
            tracking_error=tracking_error,
            var_95=var_95,
            downside_deviation=downside_deviation,
            sortino_ratio=sortino_ratio,
            calmar_ratio=calmar_ratio,
            num_rebalances=num_rebalances,
            avg_turnover=avg_turnover,
            total_costs=total_costs,
            portfolio_values=portfolio_values[1:],  # Exclure valeur initiale
            benchmark_values=benchmark_values[1:],
            dates=dates,
            rebalancing_dates=rebalancing_dates,
            backtest_start=dates[0] if dates else datetime.now(),
            backtest_end=dates[-1] if dates else datetime.now(),
            backtest_days=len(dates)
        )
    
    async def generate_intelligent_alerts(
        self,
        holdings: List[Dict[str, Any]],
        risk_metrics: Optional[RiskMetrics] = None,
        correlation_matrix: Optional[CorrelationMatrix] = None
    ) -> List[RiskAlert]:
        """
        G√©n√®re des alertes intelligentes bas√©es sur l'analyse compl√®te du portfolio
        
        Args:
            holdings: Holdings actuels
            risk_metrics: M√©triques de risque (calcul√©es si non fournies)
            correlation_matrix: Matrice de corr√©lation (calcul√©e si non fournie)
            
        Returns:
            Liste des alertes g√©n√©r√©es
        """
        try:
            logger.info("G√©n√©ration d'alertes intelligentes")
            
            # Calcul des m√©triques si non fournies
            if risk_metrics is None:
                risk_metrics = await self.calculate_portfolio_risk_metrics(holdings)
            
            if correlation_matrix is None:
                correlation_matrix = await self.calculate_correlation_matrix(holdings)
            
            # Nettoyer les alertes expir√©es
            self.alert_system.cleanup_expired_alerts()
            
            alerts = []
            
            # 1. Alertes de seuils de risque
            alerts.extend(self._check_risk_threshold_alerts(risk_metrics))
            
            # 2. Alertes de performance
            alerts.extend(self._check_performance_alerts(risk_metrics))
            
            # 3. Alertes de corr√©lation/diversification
            alerts.extend(self._check_correlation_alerts(correlation_matrix))
            
            # 4. Alertes de concentration
            alerts.extend(self._check_concentration_alerts(holdings))
            
            # 5. Alertes de qualit√© de donn√©es
            alerts.extend(self._check_data_quality_alerts(risk_metrics, correlation_matrix))
            
            # 6. Alertes de rebalancing (si applicable)
            alerts.extend(await self._check_rebalancing_alerts(holdings))
            
            logger.info(f"G√©n√©r√©es {len(alerts)} nouvelles alertes")
            
            return alerts
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration alertes: {e}")
            return []
    
    def _check_risk_threshold_alerts(self, risk_metrics: RiskMetrics) -> List[RiskAlert]:
        """V√©rifie les d√©passements de seuils de risque"""
        
        alerts = []
        thresholds = self.alert_system.thresholds
        
        # VaR 95%
        var_95 = risk_metrics.var_95_1d
        if var_95 > thresholds["var_95"]["critical"]:
            alerts.append(self.alert_system.generate_alert(
                "var_95_critical", AlertSeverity.CRITICAL, AlertCategory.RISK_THRESHOLD,
                "VaR Critique D√©pass√©",
                f"VaR 95% √† {var_95:.1%} d√©passe le seuil critique de {thresholds['var_95']['critical']:.1%}",
                "R√©duction imm√©diate de l'exposition au risque recommand√©e",
                var_95, thresholds["var_95"]["critical"]
            ))
        elif var_95 > thresholds["var_95"]["high"]:
            alerts.append(self.alert_system.generate_alert(
                "var_95_high", AlertSeverity.HIGH, AlertCategory.RISK_THRESHOLD,
                "VaR √âlev√©",
                f"VaR 95% √† {var_95:.1%} d√©passe le seuil √©lev√© de {thresholds['var_95']['high']:.1%}",
                "Surveiller √©troitement et consid√©rer r√©duction du risque",
                var_95, thresholds["var_95"]["high"]
            ))
        elif var_95 > thresholds["var_95"]["medium"]:
            alerts.append(self.alert_system.generate_alert(
                "var_95_medium", AlertSeverity.MEDIUM, AlertCategory.RISK_THRESHOLD,
                "VaR Mod√©r√©",
                f"VaR 95% √† {var_95:.1%} d√©passe le seuil mod√©r√© de {thresholds['var_95']['medium']:.1%}",
                "R√©vision de la strat√©gie de risque conseill√©e",
                var_95, thresholds["var_95"]["medium"]
            ))
        
        # Volatilit√©
        volatility = risk_metrics.volatility_annualized
        if volatility > thresholds["volatility"]["critical"]:
            alerts.append(self.alert_system.generate_alert(
                "volatility_critical", AlertSeverity.CRITICAL, AlertCategory.RISK_THRESHOLD,
                "Volatilit√© Extr√™me",
                f"Volatilit√© annualis√©e √† {volatility:.1%} d√©passe le seuil critique",
                "Rebalancing vers assets moins volatils urgent",
                volatility, thresholds["volatility"]["critical"]
            ))
        elif volatility > thresholds["volatility"]["high"]:
            alerts.append(self.alert_system.generate_alert(
                "volatility_high", AlertSeverity.HIGH, AlertCategory.RISK_THRESHOLD,
                "Volatilit√© √âlev√©e",
                f"Volatilit√© √† {volatility:.1%} indique un portfolio tr√®s risqu√©",
                "Consid√©rer diversification vers assets moins volatils",
                volatility, thresholds["volatility"]["high"]
            ))
        
        # Drawdown actuel
        current_dd = risk_metrics.current_drawdown
        if current_dd > thresholds["current_drawdown"]["critical"]:
            alerts.append(self.alert_system.generate_alert(
                "drawdown_critical", AlertSeverity.CRITICAL, AlertCategory.RISK_THRESHOLD,
                "Drawdown Critique",
                f"Drawdown actuel de {current_dd:.1%} n√©cessite action imm√©diate",
                "Stop-loss ou hedging recommand√© pour limiter les pertes",
                current_dd, thresholds["current_drawdown"]["critical"]
            ))
        elif current_dd > thresholds["current_drawdown"]["high"]:
            alerts.append(self.alert_system.generate_alert(
                "drawdown_high", AlertSeverity.HIGH, AlertCategory.RISK_THRESHOLD,
                "Drawdown Important", 
                f"Drawdown de {current_dd:.1%} indique des pertes significatives",
                "Surveillance rapproch√©e et pr√©paration de mesures d√©fensives",
                current_dd, thresholds["current_drawdown"]["high"]
            ))
        
        return alerts
    
    def _check_performance_alerts(self, risk_metrics: RiskMetrics) -> List[RiskAlert]:
        """V√©rifie les alertes li√©es √† la performance"""
        
        alerts = []
        thresholds = self.alert_system.thresholds
        
        # Sharpe ratio n√©gatif
        sharpe = risk_metrics.sharpe_ratio
        if sharpe < thresholds["sharpe_ratio"]["high"]:
            alerts.append(self.alert_system.generate_alert(
                "sharpe_very_negative", AlertSeverity.HIGH, AlertCategory.PERFORMANCE,
                "Sharpe Ratio Tr√®s N√©gatif",
                f"Sharpe ratio de {sharpe:.2f} indique performance tr√®s mauvaise vs risque",
                "R√©vision compl√®te de la strat√©gie d'investissement n√©cessaire",
                sharpe, thresholds["sharpe_ratio"]["high"]
            ))
        elif sharpe < thresholds["sharpe_ratio"]["medium"]:
            alerts.append(self.alert_system.generate_alert(
                "sharpe_negative", AlertSeverity.MEDIUM, AlertCategory.PERFORMANCE,
                "Performance N√©gative",
                f"Sharpe ratio n√©gatif ({sharpe:.2f}) indique rendement inf√©rieur au risque libre",
                "Optimisation du ratio risque/rendement recommand√©e",
                sharpe, thresholds["sharpe_ratio"]["medium"]
            ))
        
        # Max drawdown excessif
        max_dd = risk_metrics.max_drawdown
        if max_dd > thresholds["max_drawdown"]["critical"]:
            alerts.append(self.alert_system.generate_alert(
                "max_drawdown_critical", AlertSeverity.CRITICAL, AlertCategory.PERFORMANCE,
                "Drawdown Historique Extr√™me",
                f"Maximum drawdown de {max_dd:.1%} indique risque de ruine √©lev√©",
                "R√©vision fondamentale de la gestion du risque requise",
                max_dd, thresholds["max_drawdown"]["critical"]
            ))
        elif max_dd > thresholds["max_drawdown"]["high"]:
            alerts.append(self.alert_system.generate_alert(
                "max_drawdown_high", AlertSeverity.HIGH, AlertCategory.PERFORMANCE,
                "Drawdown Historique √âlev√©",
                f"Max drawdown de {max_dd:.1%} sugg√®re strat√©gie trop agressive",
                "Impl√©mentation de stop-loss et diversification renforc√©e",
                max_dd, thresholds["max_drawdown"]["high"]
            ))
        
        return alerts
    
    def _check_correlation_alerts(self, correlation_matrix: CorrelationMatrix) -> List[RiskAlert]:
        """V√©rifie les alertes de corr√©lation et diversification"""
        
        alerts = []
        thresholds = self.alert_system.thresholds
        
        # Faible diversification
        div_ratio = correlation_matrix.diversification_ratio
        if div_ratio < thresholds["diversification_ratio"]["high"]:
            alerts.append(self.alert_system.generate_alert(
                "diversification_very_low", AlertSeverity.HIGH, AlertCategory.CORRELATION,
                "Diversification Tr√®s Faible",
                f"Ratio de diversification de {div_ratio:.2f} indique assets tr√®s corr√©l√©s",
                "Ajout urgent d'assets non-corr√©l√©s ou de classes d'actifs diff√©rentes",
                div_ratio, thresholds["diversification_ratio"]["high"]
            ))
        elif div_ratio < thresholds["diversification_ratio"]["medium"]:
            alerts.append(self.alert_system.generate_alert(
                "diversification_low", AlertSeverity.MEDIUM, AlertCategory.CORRELATION,
                "Diversification Insuffisante",
                f"Ratio de {div_ratio:.2f} sugg√®re un manque de diversification",
                "Rechercher des assets moins corr√©l√©s pour am√©liorer la diversification",
                div_ratio, thresholds["diversification_ratio"]["medium"]
            ))
        
        # Corr√©lations extr√™mes (>90% entre assets principaux)
        high_correlations = []
        for asset1, correlations in correlation_matrix.correlations.items():
            for asset2, corr in correlations.items():
                if asset1 != asset2 and abs(corr) > 0.90:
                    high_correlations.append((asset1, asset2, corr))
        
        if len(high_correlations) > 2:  # Plus de 2 paires tr√®s corr√©l√©es
            affected_assets = list(set([pair[0] for pair in high_correlations] + [pair[1] for pair in high_correlations]))
            alerts.append(self.alert_system.generate_alert(
                "extreme_correlations", AlertSeverity.MEDIUM, AlertCategory.CORRELATION,
                "Corr√©lations Extr√™mes D√©tect√©es",
                f"Plusieurs paires d'assets avec corr√©lation >90%: {len(high_correlations)} paires",
                "Remplacer certains assets par des alternatives moins corr√©l√©es",
                len(high_correlations), 2,
                affected_assets
            ))
        
        return alerts
    
    def _check_concentration_alerts(self, holdings: List[Dict[str, Any]]) -> List[RiskAlert]:
        """V√©rifie les alertes de concentration"""
        
        alerts = []
        thresholds = self.alert_system.thresholds
        
        total_value = sum(h.get("value_usd", 0) for h in holdings)
        if total_value <= 0:
            return alerts
        
        # V√©rifier concentration par asset individuel
        for holding in holdings:
            weight = holding.get("value_usd", 0) / total_value
            symbol = holding.get("symbol", "Unknown")
            
            if weight > thresholds["concentration"]["critical"]:
                alerts.append(self.alert_system.generate_alert(
                    f"concentration_{symbol}_critical", AlertSeverity.CRITICAL, AlertCategory.CONCENTRATION,
                    "Concentration Critique",
                    f"{symbol} repr√©sente {weight:.1%} du portfolio (seuil critique d√©pass√©)",
                    "Diversification imm√©diate requise pour r√©duire le risque de concentration",
                    weight, thresholds["concentration"]["critical"],
                    [symbol]
                ))
            elif weight > thresholds["concentration"]["high"]:
                alerts.append(self.alert_system.generate_alert(
                    f"concentration_{symbol}_high", AlertSeverity.HIGH, AlertCategory.CONCENTRATION,
                    "Forte Concentration",
                    f"{symbol} repr√©sente {weight:.1%} du portfolio",
                    "Consid√©rer r√©duire l'exposition pour am√©liorer la diversification",
                    weight, thresholds["concentration"]["high"],
                    [symbol]
                ))
        
        # V√©rifier concentration par groupe
        from services.taxonomy import Taxonomy
        taxonomy = Taxonomy.load()
        
        group_weights = {}
        for holding in holdings:
            alias = holding.get("alias", holding.get("symbol", ""))
            group = taxonomy.group_for_alias(alias)
            weight = holding.get("value_usd", 0) / total_value
            group_weights[group] = group_weights.get(group, 0) + weight
        
        for group, weight in group_weights.items():
            if weight > thresholds["concentration"]["high"]:
                group_assets = [h.get("symbol", "") for h in holdings 
                              if taxonomy.group_for_alias(h.get("alias", h.get("symbol", ""))) == group]
                
                alerts.append(self.alert_system.generate_alert(
                    f"group_concentration_{group}", AlertSeverity.MEDIUM, AlertCategory.CONCENTRATION,
                    f"Concentration Groupe {group}",
                    f"Groupe {group} repr√©sente {weight:.1%} du portfolio",
                    "Diversifier vers d'autres groupes d'assets pour r√©duire le risque sectoriel",
                    weight, thresholds["concentration"]["high"],
                    group_assets
                ))
        
        return alerts
    
    def _check_data_quality_alerts(self, risk_metrics: RiskMetrics, correlation_matrix: CorrelationMatrix) -> List[RiskAlert]:
        """V√©rifie la qualit√© et fiabilit√© des donn√©es"""
        
        alerts = []
        
        # Confiance faible dans les calculs
        if risk_metrics.confidence_level < 0.5:
            alerts.append(self.alert_system.generate_alert(
                "data_confidence_low", AlertSeverity.MEDIUM, AlertCategory.DATA_QUALITY,
                "Confiance Faible dans les Donn√©es",
                f"Niveau de confiance de {risk_metrics.confidence_level:.1%} indique donn√©es insuffisantes",
                "Collecter plus de donn√©es historiques pour am√©liorer la fiabilit√©",
                risk_metrics.confidence_level, 0.5
            ))
        
        # Donn√©es manquantes
        if risk_metrics.data_points < 10:
            alerts.append(self.alert_system.generate_alert(
                "insufficient_data", AlertSeverity.HIGH, AlertCategory.DATA_QUALITY,
                "Donn√©es Insuffisantes",
                f"Seulement {risk_metrics.data_points} points de donn√©es disponibles",
                "Minimum 30 points recommand√©s pour des calculs de risque fiables",
                risk_metrics.data_points, 30
            ))
        
        return alerts
    
    async def _check_rebalancing_alerts(self, holdings: List[Dict[str, Any]]) -> List[RiskAlert]:
        """V√©rifie si un rebalancing est n√©cessaire"""
        
        alerts = []
        
        try:
            # Calculer la d√©rive par rapport aux allocations cibles (exemple simple)
            # En pratique, il faudrait avoir des allocations cibles d√©finies
            
            # Pour la d√©mo, v√©rifier si certains assets ont beaucoup √©volu√©
            total_value = sum(h.get("value_usd", 0) for h in holdings)
            if total_value <= 0:
                return alerts
            
            # G√©n√©rer returns r√©cents simul√©s pour estimer la d√©rive
            recent_returns = await self._generate_historical_returns(holdings, 7)  # 7 derniers jours
            
            large_moves = []
            for holding in holdings:
                symbol = holding.get("symbol", "")
                if symbol in recent_returns[-1]:  # Dernier jour
                    recent_return = sum(day.get(symbol, 0.0) for day in recent_returns[-7:])  # Return 7 jours
                    if abs(recent_return) > 0.20:  # Mouvement > 20% en 7 jours
                        large_moves.append((symbol, recent_return))
            
            if len(large_moves) >= 3:  # Plus de 3 assets avec gros mouvements
                affected_symbols = [move[0] for move in large_moves]
                alerts.append(self.alert_system.generate_alert(
                    "rebalancing_needed", AlertSeverity.MEDIUM, AlertCategory.REBALANCING,
                    "Rebalancing Recommand√©",
                    f"{len(large_moves)} assets ont eu des mouvements >20% cette semaine",
                    "Consid√©rer un rebalancing pour maintenir les allocations cibles",
                    len(large_moves), 3,
                    affected_symbols
                ))
            
        except Exception as e:
            logger.warning(f"Erreur v√©rification rebalancing: {e}")
        
        return alerts

    def get_system_status(self) -> Dict[str, Any]:
        """
        Obtient le statut du syst√®me de gestion des risques
        
        Returns:
            Dict avec les informations de statut
        """
        try:
            return {
                "status": "operational",
                "risk_manager_initialized": True,
                "cache_size": len(self.risk_metrics_cache),
                "alert_system_active": True,
                "active_alerts_count": len(self.alert_system.get_active_alerts()),
                "supported_scenarios": list(self.stress_scenarios.keys()),
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error getting system status: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

# Instance globale du gestionnaire de risques
risk_manager = AdvancedRiskManager()
